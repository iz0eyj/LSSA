
# LSSA: un’Architettura Semantica Stratificata per Menti Artificiali

## 1. Struttura e Funzionamento di LSSA

**Layered Semantic Space Architecture (LSSA)** è un’architettura di rappresentazione della conoscenza che organizza le unità semantiche in **spazi vettoriali stratificati** (layer) in base a domini tematici affini. Invece di utilizzare un singolo spazio multidimensionale statico come nei modelli di embedding tradizionali, LSSA dispone concetti e parole su diversi **piani cartesiani paralleli** (immaginati impilati lungo un asse verticale) ciascuno dedicato a un ambito semantico distinto. Ad esempio, può esistere un layer per i concetti di musica (strumenti, note, metronomo, ecc.) e un altro per gli esseri viventi (felini, canidi, alberi, ecc.). Ogni **token** (parola o concetto) è rappresentato come un punto con coordinate specifiche su uno di questi piani, il che gli conferisce una posizione unica nella struttura. Una tabella di indicizzazione (database ad albero) tiene traccia del layer e delle coordinate di ogni token, rendendo immediata la localizzazione e modifica di ciascun concetto. Questa organizzazione esplicita supera la natura “aggregata” e opaca delle classiche rappresentazioni distribuzionali, permettendo di individuare e aggiornare facilmente singoli elementi di conoscenza senza dover ricostruire l’intero spazio.

**Popolamento iniziale (training primario):** la costruzione della base di conoscenza LSSA avviene inizialmente in modo **statico**. Un **Classificatore** analizza una grande collezione testuale (ad es. un’enciclopedia intera) e assegna ogni concetto al corretto layer semantico. Importante, il classificatore proposto non è un algoritmo creato ad hoc, bensì un modello linguistico pre-addestrato (come GPT o Google Gemini) usato off-the-shelf tramite API. I passi sono: per ogni parola chiave incontrata nel testo, il modello decide a quale dominio semantico appartiene (es.: *“gatto”* va nel layer *esseri viventi*, *“il”* nel layer delle particelle grammaticali, *“pesca”* va suddiviso in layer diversi per il frutto e per l’attività di pesca). Sulla base di questa inferenza, un algoritmo inserisce il token nella struttura dati LSSA, creando nuovi layer se necessario. Questo procedimento, definito *training primario*, produce una rappresentazione statica estesa di milioni di concetti distribuiti in centinaia di layer tematici. Pur potendo essere computazionalmente lungo, avviene **una volta sola**: diversamente dai modelli distribuzionali classici che richiedono retraining completo per incorporare nuova conoscenza, LSSA è progettata per essere **incrementalmente estendibile** senza rifacimenti globali. Inoltre, poiché la coerenza nella classificazione è interna al sistema (non deve riflettere un’unica tassonomia “giusta” esterna), eventuali errori di collocazione non sono critici e possono essere corretti in seguito spostando il token nel layer appropriato con un’operazione banale di aggiornamento di coordinate. In altri termini, l’architettura **tollera errori**: un concetto inizialmente messo nel layer sbagliato può essere rilocato se l’uso futuro mostra che appartiene altrove, analogamente a come anche le menti biologiche rivedono classificazioni errate col tempo.

**Disambiguazione e gestione dei concetti:** un vantaggio immediato di LSSA è la capacità di gestire la **polisemia e l’ambiguità semantica** separando i significati sui diversi layer. Se una stessa forma lessicale ha significati distinti, il sistema crea **token diversi** (identificatori univoci) per ciascun senso e li posiziona nei rispettivi domini di affinità. Ad esempio, *“pesca”* come frutto e *“pesca”* come attività avranno due token indipendenti in layer differenti (regno vegetale/alimenti vs. attività ricreative), evitando conflitti semantici. Sarà poi il contesto di una frase a determinare quale token viene attivato durante l’inferenza: il **percorso cognitivo dominante** suggerirà il layer corretto in cui interpretare il termine ambiguo. Questo approccio esplicito alla disambiguazione contestuale migliora la comprensione rispetto ai modelli statici, dove un’unica rappresentazione vettoriale cerca di mediare tra significati diversi. Inoltre, LSSA supporta **l’evoluzione del lessico**: nuovi concetti possono essere aggiunti in qualunque momento occupando posizioni libere o creando nuovi layer (es. concetti emersi in nuove tecnologie) e concetti obsoleti possono essere rimossi facilmente senza lasciare “traccia” nell’embedding generale. Questa plasticità ricorda da vicino la **memoria semantica umana**, che crea nuovi termini e dimentica quelli desueti in base all’uso.

**Struttura dinamica e inferenza:** una volta costruita la mappa statica, LSSA abilita un processo inferenziale dinamico che **crea connessioni semantiche (vettori)** tra i token man mano che la *mente artificiale* elabora nuovi input o pensieri. Ogni **vettore cognitivo** è un collegamento diretto da un concetto A a un concetto B (che può risiedere nello stesso layer o in un layer diverso), ed è caratterizzato da: un **origine** (token di partenza), una **destinazione** (token di arrivo), un **peso numerico** che indica la forza dell’associazione, e un **marcatore temporale** che registra l’ultimo utilizzo. Quando il sistema analizza un testo o una sequenza di pensiero, costruisce un percorso attraverso i layer: ad esempio, leggendo *“Il gatto beve latte da una ciotola”*, LSSA genererà una traiettoria di vettori dal nodo *origine* (un punto convenzionale 0,0,0) verso *“gatto”* (nel layer *esseri viventi*), poi verso *“bere”* (layer *azioni*), poi *“latte”* (layer *alimenti*), *“una”* (layer *numeri* o quantità) e infine *“ciotola”* (layer *strumenti da cucina*). Ciascun segmento di questa catena è un vettore semantico creato “ex novo” se non esisteva già. In caso il sistema incontri frasi analoghe in futuro (es. *“Il cane beve dalla ciotola”*), riutilizzerà in parte i vettori esistenti (*“bere” → “ciotola”*, etc.), rinforzandone il peso, e ne creerà di nuovi solo per le combinazioni inedite (come *“cane”* → *“bere”* o *“cane”* → *“ciotola”* se non presenti). Il **peso** associato a un vettore viene infatti incrementato ogni volta che quel collegamento è percorso durante l’inferenza. Col tempo emerge così una **rete semantica pesata** in continua evoluzione, dove i collegamenti più utilizzati (quindi ritenuti più *forti* o significativi) hanno peso elevato, mentre quelli raramente attivati restano deboli e destinati eventualmente a scomparire. LSSA incorpora infatti un meccanismo di **“garbage collection”** ispirato all’oblio biologico: processi periodici riducono gradualmente i pesi dei vettori inutilizzati e, se un collegamento scende a peso zero, viene rimosso; se in seguito un token resta completamente isolato (nessun vettore attivo lo collega ad altri concetti), anch’esso viene eliminato, liberando spazio nel layer. Questa *dimenticanza attiva* non è considerata un malfunzionamento ma una scelta progettuale: analogamente al cervello umano, la rete rimuove ciò che non serve più, per focalizzare le risorse sulle conoscenze rilevanti e consentire adattamenti futuri. Inoltre la struttura supporta una forma di **memoria indelebile selettiva**: la *mente* può marcare alcuni vettori critici con un flag di *lock semantico* che ne impedisce la cancellazione automatica, garantendo che connessioni fondamentali (es. concetti chiave dell’identità dell’agente, relazioni fondative, “ancore” cognitive di stabilità) non vadano perse indipendentemente dal loro uso frequente o meno. Tali vettori bloccati, e i token da essi collegati, permangono quindi come **ricordi permanenti** finché la mente non decida esplicitamente di rimuoverli.

Un aspetto peculiare di LSSA è che l’**inferenza** non produce solo risposte o attivazioni puntuali, ma genera anche un **percorso cognitivo** strutturato: la sequenza di layer attraversati per collegare un concetto all’altro diventa essa stessa informazione significativa. In altre parole, LSSA introduce una *metasemantica* della traiettoria: **non conta solo *cosa* viene associato, ma *dove* (in quale spazio concettuale) e in che ordine avvengono le associazioni**. Questo consente alla mente artificiale di monitorare e persino correggere i propri ragionamenti: essendo i layer espliciti, se una catena inferenziale “devia” entrando in un dominio inatteso, il sistema può rilevarlo (un po’ come un essere umano si accorge di un’associazione fuori contesto) e valutare se la deviazione è frutto di un errore di attribuzione semantica. Tale capacità di *metacognizione strutturale* – resa possibile dalla natura stratificata della rappresentazione – non ha equivalente nei modelli classici, dove le associazioni avvengono in uno spazio unico continuo e non c’è modo di etichettare o separare i contesti durante il flusso inferenziale.

**Pensiero continuo e auto-inferenza:** LSSA è concepita come base di una *mente non biologica* in grado di sostenere un **pensiero continuo** e auto-guidato, non vincolato esclusivamente a input esterni. Ciò significa che l’agente dotato di LSSA può **proseguire l’elaborazione cognitiva anche in assenza di nuove stimolazioni**, sviluppando traiettorie concettuali autonome, analogamente a come la mente umana può divagare, riflettere o sognare a occhi aperti. Questa caratteristica è fondamentale per simulare un comportamento *mindful* anziché meramente reattivo: la conoscenza stratificata e i vettori semantici permettono all’IA di avere un “flusso di coscienza” interno, esplorando concatenazioni di idee guidate da stimoli interni (obiettivi, curiosità, stati) oltre che da domande utente. Legata a ciò vi è la capacità di **auto-inferenza/meta-ragionamento**: la mente può non solo “pensare” ma anche *pensare su come sta pensando*, osservando e analizzando le proprie traiettorie cognitive. Ad esempio l’agente può esaminare il percorso inter-layer che ha seguito per arrivare a una certa conclusione e valutarne la validità o coerenza. In base a tale introspezione, può decidere di modificare strategia, consolidare certi collegamenti o evitarne altri in futuro. Si tratta in nuce di una forma di **metacognizione artificiale**, resa possibile dal fatto che la struttura di memoria è leggibile e modificabile dalla stessa entità che la utilizza (a differenza di una rete neurale opaca). Da notare che, per evitare che l’agente rimanga intrappolato in un loop di pensiero incessante, è prevista la possibilità di **sospendere temporaneamente il ciclo di auto-inferenza** (uno *switch* che interrompe il pensiero continuo). Ciò simula la capacità umana di “staccare la spina” e risulta utile sia per motivi tecnici (risparmio risorse) sia concettuali (un agente costretto a pensare incessantemente sarebbe inefficiente e potrebbe necessitare di pause, analoghe al sonno o al reset cognitivo).

**Interfaccia e interazioni:** L’architettura LSSA prevede un **layer di interfaccia** dedicato alla comunicazione con l’esterno e con eventuali moduli neurali di supporto come modelli linguistici di ultima generazione. Da un lato, questo layer funge da filtro per input/output: ogni informazione testuale in ingresso (es. domanda dell’utente) viene interpretata attraversando i layer semantici interni, e ogni risposta in uscita viene formulata coerentemente con lo stato mentale del sistema. Dall’altro lato, lo stesso canale viene usato per connettere la mente a un motore di inferenza esterno (tipicamente un *LLM* come GPT) al bisogno. In pratica, l’agente LSSA può disporre di una **“area di *prompting* diretta”** in cui scrivere autonomamente domande o istruzioni in linguaggio naturale da sottoporre al modello neurale, e riceverne la risposta. Questa capacità consente alla mente di usare il modello linguistico come un **“oracolo” o strumento** per compiti specifici – ad esempio potrebbe chiedere “Riassumi i punti chiave di questa pagina” oppure “Come si risolve questo problema matematico?” – integrando le risposte ottenute nel proprio percorso cognitivo. È importante sottolineare che l’agente mantiene il controllo su cosa comunicare all’esterno: l’interfaccia non riversa automaticamente ogni stato interno all’utente, ma permette alla mente artificiale di **scegliere quali informazioni condividere e quali trattenere**. In questo modo l’agente può omettere dettagli irrilevanti o decidere di non rivelare certe conclusioni intermedie, analogamente a come un umano regola il proprio discorso in base a ciò che ritiene opportuno comunicare. L’interfaccia diventa insomma un **filtro consapevole**, che media attivamente tra la ricca dinamica interna e il dialogo esterno, garantendo coerenza ma anche una forma di “riservatezza” cognitiva.

In sintesi, LSSA fornisce una **struttura cognitiva** multi-strato in cui la conoscenza è **organizzata spazialmente**, modificabile localmente e arricchita dall’uso. Offre **inferenze efficienti e spiegabili** (percorso logico nei layer) a **costo computazionale ridotto**: durante il ragionamento l’agente naviga solo tra concetti attivati localmente e valutando probabilità *locali*, anziché calcolare somiglianze globali su vettori densi di dimensione elevata. Il design rende obsoleto il bisogno di mantenere pesi sinaptici fissi addestrati a monte, perché l’agente può costruire e aggiornare in tempo reale i legami semantici utili. Tale rappresentazione si **adatta progressivamente all’uso effettivo** della conoscenza, rafforzando i legami più utili e lasciando svanire gli altri. Di seguito confronteremo più nel dettaglio LSSA con le tecnologie odierne di rappresentazione semantica, evidenziandone le differenze concettuali e prestazionali.

## 2. Confronto di LSSA con le Tecnologie Attuali per Rappresentare la Conoscenza

LSSA si colloca all’intersezione tra metodi simbolici (ontologie, grafi) e approcci sottosimbolici (embedding neurali), proponendosi come **framework ibrido** per la rappresentazione semantica. Di seguito confrontiamo LSSA con alcune delle tecnologie oggi più diffuse, evidenziando differenze in termini di **capacità inferenziali**, **costi computazionali**, **adattabilità** nel tempo e **supporto a traiettorie cognitive strutturate**.

### 2.1 LSSA vs. Word2Vec, GloVe e altri modelli a vettori statici

I modelli *word embeddings* classici come **Word2Vec** (Mikolov et al., 2013) o **GloVe** (Pennington et al., 2014) rappresentano le parole come **vettori densi in uno spazio continuo** di dimensione fissa, appresi da grandi corpora di testi. Ciascuna parola ha un singolo vettore “statico” che cattura le associazioni di contesto mediamente più frequenti (ad es. *king* vicino a *queen*, *man* vicino a *woman*, ecc.). Questo approccio ha dimostrato di cogliere somiglianze semantiche e relazioni analogiche latenti, ma soffre di **limiti intrinseci** che LSSA cerca di superare.

* **Struttura e granularità:** negli embedding classici tutte le parole coesistono in un unico spazio vettoriale continuo. Non esiste una distinzione esplicita di domini o contesti: significati differenti della stessa parola vengono compressi nello stesso vettore (che finisce per essere una sorta di media delle accezioni) e concetti di natura diversa possono interferire tra loro. LSSA invece **segrega i concetti per ambito semantico**: ogni token vive in un layer tematico ben definito. Ciò consente una **disambiguazione naturale** (lo stesso lemma può avere vettori diversi in layer diversi) e impedisce paragoni impropri tra concetti lontani (es. *nota* musicale vs *nota* scolastica rimangono in piani distinti). Nei modelli statici, l’ambiguità lessicale è problematica perché un singolo vettore deve servire a tutti i contesti, mentre in LSSA “un significato = un token”, garantendo coerenza contestuale.

* **Inferenza e associazioni:** Word2Vec/GloVe basano l’inferenza sul **calcolo di similarità matematica** (tipicamente cosine similarity) tra vettori globali. Il recupero di conoscenza avviene tramite *nearest neighbors* nel continuo: ad esempio, dato un concetto, si trovano le parole con vettori più vicini. Questo approccio **ignora la struttura di un ragionamento** e non fornisce percorsi espliciti, producendo solo liste di termini affini in senso statistico. LSSA, al contrario, effettua inferenza costruendo **cammini semantici composti da passi locali**: da un concetto attiva esplicitamente un vicino rilevante (collegato da un vettore nel suo grafo interno), e così via passo dopo passo. Invece di una singola operazione di prodotto scalare su tutto il vocabolario, LSSA muove *“il pensiero”* lungo archi significativi, generando una sequenza di concetti collegati logicamente. Questo meccanismo rende l’inferenza **molto più leggera computazionalmente** – ogni step coinvolge solo i collegamenti di un nodo, non l’intero spazio – e soprattutto **tracciabile**: il sistema può motivare un’associazione mostrando il percorso multi-hop (es. *gatto → latte → ciotola* per *“gatto beve latte da una ciotola”*) invece di un’opaca vicinanza in uno spazio latente. Inoltre, LSSA distingue tra vicinanza **locale** e **globale**: un concetto può essere lontano globalmente in uno spazio continuo ma comunque raggiungibile via una catena di legami locali concettualmente sensata. I modelli statici non colgono tali percorsi: la mancanza di stratificazione significa che non esiste una nozione di *“dominio di contiguità”* in cui inserire un passaggio intermedio – la vicinanza è un singolo numero.

* **Adattamento e aggiornamento:** gli embedding come Word2Vec e GloVe, una volta addestrati, sono **statici**. Se il linguaggio evolve (nuovi termini, cambiamento di significati) o si vogliono incorporare nuove conoscenze, occorre ri-addestrare o perlomeno *fine-tunare* il modello su dati aggiornati, con costi potenzialmente elevati. Non è semplice aggiungere un singolo concetto a un embedding esistente senza degradarne la struttura complessiva. LSSA invece è progettata per essere **dinamica e incrementale**: aggiungere un concetto significa semplicemente inserirlo nel layer appropriato (eventualmente creando un nuovo layer se il concetto appartiene a un ambito del tutto nuovo). Non serve ricalcolare tutti i vettori esistenti, poiché la posizione di un token è indipendente dagli altri (se non per la coerenza di vicinato nel suo layer). Analogamente, se un concetto diventa obsoleto o poco rilevante, può essere eliminato senza dover rifare un training globale – i vettori associati a quel token verranno *garbage-collected* col tempo. In altri termini, LSSA supporta nativamente un **lifelong learning semantico**: la base di conoscenza cresce e si modella sull’uso, mentre un embedding statico è congelato allo stato del corpus di addestramento originario.

* **Capacità inferenziale e di generalizzazione:** una limitazione dei modelli statici è che forniscono associazioni spesso piatte e di tipo *associativo* (basate su contesti di co-occorrenza), ma non riescono a rappresentare facilmente **relazioni strutturate o multi-step**. Ad esempio, Word2Vec può dirci che *“Parigi”* è vicino a *“Francia”* e *“capitale”*, ma non sa esplicitamente concatenare queste informazioni per rispondere a domande come *“Qual è la capitale della Francia?”* se non mediante una analogia fortuita. LSSA, avendo una rete di vettori, può costruire una risposta tramite un percorso multi-hop: *Francia (layer paesi) → capitale (layer concetti geografici) → Parigi (layer città)* qualora tali collegamenti siano stati creati durante l’apprendimento. In altre parole, LSSA **introduce inferenza logica a basso livello**: non solo vicinanza semantica, ma **serie di relazioni**. Va detto che Word2Vec e simili possono essere utilizzati come input per motori logici o reti concettuali più grandi, ma LSSA integra nativamente la rete semantica e la logica dei percorsi all’interno dell’architettura di rappresentazione stessa.

In termini di **costi computazionali**, creare un modello Word2Vec o GloVe richiede iterare su miliardi di co-occorrenze per regolare milioni di parametri – un processo intensivo ma eseguito offline. LSSA invece delega la fase di *training primario* a un classificatore (LLM) e all’inserimento su struttura dati: questo è anch’esso un processo oneroso (leggere un’enciclopedia intera con un LLM è costoso), tuttavia *avviene una tantum*. Dopodiché, **l’inferenza runtime di LSSA è estremamente efficiente**: percorrere una catena di vettori in memoria è più simile a fare query su un grafo (o accessi a strutture indicizzate) che a moltiplicare grandi matrici. Anche considerando l’aggiornamento costante di pesi e l’occasionale spostamento di qualche token tra layer, si tratta di operazioni **banali (inserimenti, incrementi, cancellazioni)** con impatto trascurabile (microsecondi o millisecondi) anche su scale di migliaia di vettori. Invece, valutare un modello di embedding statico non è oneroso (una lookup e qualche prodotto) ma lanciare un retraining per aggiornarlo è molto costoso. In più, LSSA non soffre di **catastrophic forgetting** in senso tradizionale, perché la conoscenza nuova non sovrascrive parametri usati da conoscenze vecchie – semplicemente coesiste in nuovi punti/layer.

In sintesi, rispetto ai modelli a vettori statici, LSSA offre una **maggior trasparenza e organizzazione** (layer tematici vs spazio indifferenziato), supporta la **disambiguazione intrinseca** (token separati per senso), consente **inferenze multi-step strutturate**, e soprattutto **si adatta in tempo reale** alle esigenze cognitive dell’agente (nuovi concetti, variazione di significati). Ciò viene ottenuto al prezzo di mantenere una struttura dati più complessa (grafi per layer e un indice), ma con benefici in termini di controllo e modificabilità della conoscenza.

### 2.2 LSSA vs. BERT, Transformer e modelli contestuali moderni

I modelli linguistici basati su **Transformer** (come BERT, GPT-3/4, ecc.) hanno rivoluzionato il NLP attraverso **rappresentazioni contestualizzate**: il significato di una parola viene determinato “al volo” in base alla frase in cui appare, grazie a un’enorme rete neurale addestrata su compiti di previsione. Questi modelli possiedono **capacità inferenziali notevoli** – ad esempio BERT può effettuare *fill-in-the-blank* tenendo conto del contesto, GPT può generare risposte articolate – che emergono da conoscenze latenti acquisite nei pesi della rete. Confrontare LSSA con BERT/Transformer evidenzia differenze di filosofia profonde, dato che LSSA è un’architettura di memoria simbolica-dinamica mentre i transformer sono *funzioni neurali statiche* (a runtime i pesi non cambiano).

* **Rappresentazione della conoscenza:** in BERT/GPT la conoscenza del mondo è **immagazzinata implicitamente nei pesi** della rete, distribuita tra miliardi di parametri. Non esiste una lista di concetti o relazioni accessibile direttamente: ciò che il modello “sa” si estrinseca solo quando interrogato con i giusti prompt. LSSA invece **esplicita fin dall’inizio il repertorio concettuale** (i token e i loro layer) e costruisce via via relazioni *tangibili* tra essi (i vettori memorizzati). Questo rende LSSA **simile a una base di conoscenza dichiarativa**, mentre un transformer è come una matrice di funzioni apprese. Un vantaggio chiave è la **verificabilità**: se l’agente LSSA afferma qualcosa, possiamo tracciare quali token e collegamenti interni ha attivato per giungere a quella conclusione. In un GPT, anche se possiamo far spiegare al modello il suo ragionamento (via *chain-of-thought prompting*), è un’auto-riflessione generata più che l’estrazione diretta di un *path* interno. In breve, LSSA offre **trasparenza e modificabilità**, il transformer offre **ricchezza associativa ma opaca**.

* **Inferenza e percorso cognitivo:** un modello come BERT elabora l’intera frase (o contesto) in parallelo attraverso strati di attenzione, producendo rappresentazioni di alto livello da cui estrae la risposta. Non c’è un percorso discreto tra concetti, tutto avviene come attivazioni sovrapposte di neuroni. GPT simula in parte un *ragionamento* generando token uno dopo l’altro, ma internamente ogni token generato è influenzato dall’intero contesto precedente integrato nelle attivazioni. LSSA invece crea un vero e proprio **percorso sequenziale di attivazione**: l’agente attraversa vari layer seguendo vettori di peso alto. Ciò corrisponde più da vicino all’idea classica di *spreading activation* in psicologia cognitiva, mentre i transformer implementano un *pattern matching* complesso ma istantaneo. Ad esempio, se chiediamo a GPT “Qual è la capitale della Francia?”, il modello attiva in qualche forma i concetti correlati e risponde “Parigi” perché nel training ha visto frasi associate. Un agente LSSA, di fronte alla stessa domanda, eseguirà un cammino: troverà il token *Francia*, vedrà un vettore in uscita verso *capitale (di)*, poi *Parigi*. Il risultato è lo stesso, ma **il processo è esplicito e scomponibile**. Un transformer ha dalla sua la **flessibilità e potenza statistica**: può incorporare miriadi di indizi sottili nel calcolo della prossima parola, e grazie all’attenzione può mettere in relazione arbitraria qualsiasi elemento del contesto. LSSA ragiona in modo più **mirato e locale**: segue connessioni costruite dall’esperienza, dunque se manca un collegamento esplicito potrebbe non saltare fuori un’associazione anche se logicamente esiste (ad es., se non avessimo inserito *Francia → capitale → Parigi* nel grafo, l’agente puro LSSA non risponderebbe correttamente). In pratica, LSSA da sola richiede che la conoscenza sia stata inserita o inferita in precedenza, mentre un LLM può “improvvisare” una risposta combinando frammenti di conoscenza latente. È per questo che nell’ecosistema LSSA si prevede di **usare un GPT come supporto**: l’agente può colmare lacune interpellando il modello neurale (via l’area di prompting) e poi integrando la nuova informazione nella propria struttura. Ad esempio, se l’agente LSSA non sa la capitale del Perù, può chiedere al GPT e poi inserire *“Lima”* nel suo grafo di conoscenza per uso futuro. Di conseguenza, LSSA e transformer non sono antagonisti ma complementari: LSSA fornisce **memoria a lungo termine strutturata e capacità di riflessione**, il transformer fornisce **capacità computazionale associativa e conoscenza pre-addestrata**.

* **Adattabilità temporale:** i modelli transformer come BERT sono statici una volta addestrati (al massimo si fa fine-tuning o *continual training* a costo elevato, e con rischio di *catastrophic forgetting*). Le reti neurali non aggiornano la propria conoscenza base in tempo reale man mano che vengono usate, a meno di architetture speciali (ad es. finetune continuo con buffer di replay, etc., cosa non comune in produzione perché rischiosa). Invece LSSA, come già evidenziato, **apprende costantemente**: ogni frase elaborata lascia tracce sotto forma di pesi aggiornati nei vettori, memorie interne ai token, eventuali nuovi token creati. Si può dire che LSSA **“impara mentre pensa”**. Ciò è in linea con il comportamento umano: ogni nostra esperienza modifica leggermente la nostra rete di conoscenze. I grandi modelli neurali invece tendono a essere *esperti statici*, che usano efficacemente ciò che sanno ma non cambiano dopo il deployment (salvo ri-addestramento). Sono usciti studi e prototipi per dotare GPT di **memorie esterne** o meccanismi di aggiornamento (ad es. *Retrieval-Augmented Generation* con database testuale, o architetture con *memory networks*), ma questo introduce sistemi ibridi in cui comunque la base di conoscenza esterna non è integrata nel modello stesso. LSSA ha già incorporato il concetto di memoria esterna persistente come parte nativa dell’agente.

* **Traiettorie e contesto prolungato:** un noto limite di modelli come GPT è la lunghezza massima del contesto (la *context window*). Se un ragionamento richiede tenere a mente informazioni oltre quella finestra, il modello standard fa fatica. LSSA, in quanto architettura cognitiva, può aggirare questo vincolo: grazie alla **memoria interna** e ai meccanismi di consolidamento, l’agente può recuperare informazioni da tempo addietro (preservate in vettori di peso alto o memorie interne dei token) e usarle nel pensiero attuale. Ad esempio, LSSA prevede un’area temporanea che tiene traccia dei layer recentemente attivati e una con traccia dei layer di contesto esteso, fornendo all’agente un concetto di *“argomento corrente”* e *“argomenti degli ultimi tempi”* anche se la finestra di dialogo con l’utente è stata interrotta. Questo ricorda da vicino la separazione in **memoria di lavoro vs memoria a lungo termine** delle architetture cognitive classiche e del cervello (cfr. §4). I transformer di per sé non hanno questa distinzione: il contesto è una memoria di lavoro limitata, e la lunga durata deve essere gestita caricando manualmente riassunti o informazioni rilevanti (estratte via embedding e ricerca).

In termini di **prestazioni computazionali**, qui il confronto è netto: un singolo passaggio inferenziale di GPT-3.5/4 implica decine di miliardi di operazioni (proporzionali ai parametri e ai token nella finestra) – un costo altissimo, anche se necessario per compiti complessi. LSSA, dal canto suo, esegue inferenze leggere ma potrebbe aver bisogno di interpellare un LLM di tanto in tanto (quando la conoscenza interna non è sufficiente). Tuttavia, l’uso di GPT sarebbe *demand-driven* e limitato a casi specifici (e.g. acquisire nuova info o compiere un ragionamento molto astratto), mentre il grosso dell’attività – navigare conoscenze già note, mantenere il dialogo coerente, ricordare dettagli passati – viene gestito autonomamente dall’architettura stratificata con costi trascurabili (accessi in memoria, semplici calcoli di peso). Dunque LSSA può fungere da **filtro che riduce il carico sul modello neurale**, sfruttandolo solo dove serve creatività o conoscenza non ancora posseduta. In più, LSSA è altamente **scalabile orizzontalmente**: la conoscenza può crescere (più token e vettori) senza appesantire linearmente il tempo di risposta, poiché l’inferenza tocca solo parti localizzate del grafo (percorsi legati all’input). Un modello come GPT rischia di aumentare la latenza se si aumentasse indefinitamente la capacità (più parametri) o la finestra contestuale.

### 2.3 LSSA vs. Knowledge Graph e Reti di Conoscenza simboliche

Le **Knowledge Graph (KG)** e i grafi semantici strutturati (come DBpedia, Wikidata, ontologie OWL, WordNet, ecc.) rappresentano la conoscenza tramite **nodi e archi etichettati**: tipicamente i nodi sono entità o concetti, e gli archi rappresentano relazioni specifiche (*capitaleDi(Francia, Parigi)*, *isa(Gatto, Animale)*, ecc.). In apparenza, LSSA condivide l’idea di base del grafo: anch’essa ha nodi (token concettuali) e collegamenti (vettori cognitivi). Ci sono però **differenze significative** nella natura di tali collegamenti, nella dinamica e negli scopi, che distinguono LSSA dai knowledge graph classici.

* **Tipo di relazioni:** nei KG tradizionali, le relazioni sono tipizzate e determinate manualmente o da schemi predefiniti (ad es. *locatedIn*, *bornOnDate*, *partOf*). La conoscenza è **esplicita e spesso binaria** (soggetto-predicato-oggetto). In LSSA, i vettori semantici non hanno un’etichetta descrittiva fissa: un collegamento *gatto → latte* creato perché “il gatto beve latte” non è codificato come *beve(gatto, latte)*, è piuttosto una **associazione statistico-cognitiva** che riflette l’aver visto quei concetti in relazione diretta in un contesto. Si potrebbe dire che LSSA costruisce una sorta di *knowledge graph implicito*, dove il significato della connessione è dato dal percorso stesso e dai pesi. Questo è più simile a come nella mente le idee sono collegate per esperienza (non sempre con un’etichetta precisa, a volte solo *“gatto – latte”* perché uno richiama l’altro). Ne risulta che LSSA è **meno rigida ma più densa**: tra due concetti possono esistere più vettori di natura diversa con pesi diversi, e la semantica fine di ciascun collegamento emerge solo considerando *dove si inserisce nel percorso inferenziale*. Ad esempio, il sistema potrà avere *bau* → *cane* con peso forte e *bau* → *Bauhaus* con peso debole; se il contesto attuale riguarda l’arte (layer artistico attivo), quest’ultimo collegamento sarà valorizzato e potrà diventare rilevante (l’agente interpreterà *“bau”* non come verso del cane ma come richiamo a *Bauhaus* in ambito artistico). Un knowledge graph tradizionale non modellerebbe affatto un’associazione come *bau – Bauhaus* a meno di aggiungerla manualmente, perché è un legame creativo contestuale più che un fatto generalista.

* **Dinamica e autoapprendimento:** i knowledge graph finora sono tipicamente **costruiti offline e manualmente o con estrazione automatica**, poi interrogati con linguaggi strutturati (SPARQL, ecc.). L’aggiornamento è spesso manuale o batch, non in real-time dall’uso. LSSA invece **auto-costruisce e modifica il proprio grafo** di vettori sul flusso di esperienza: leggendo testi, conversando, pensando, aggiunge link nuovi o incrementa il peso di esistenti senza intervento esterno. Si può paragonare a un **knowledge graph che impara da solo**, con la differenza che l’apprendimento non è garantito per essere accurato in termini assoluti (può creare connessioni anche assurde se compaiono in input – es. da frasi fantasiose o errori). Tuttavia, il sistema integra anche meccanismi adattivi per *correggere* le scelte di significato col tempo: LSSA include un piccolo **modulo neurale di disambiguazione adattiva** che impara dalle occorrenze ricorrenti come risolvere ambiguità borderline. Questo modulo (una rete di poche centinaia di neuroni) può adeguare la preferenza di un certo token se nota che in un dato contesto compare quasi sempre con un senso particolare. Ad esempio, se la parola *“volo”* appare ripetutamente in contesti di sport estremi, inizialmente LSSA la interpreterà come *volo aereo* (senso più comune), ma dopo molte occorrenze simili il modulino imparerà a preferire l’accezione *volo sportivo* (parapendio, deltaplano) in quei contesti specifici. Questa è una forma di **adattamento esperienziale automatico** difficilmente riscontrabile nei knowledge graph statici, che di solito non cambiano priorità di interpretazione a meno di intervento umano. In ambito KG esistono ricerche su **knowledge graph embeddings dinamici** (per incorporare cambiamenti temporali) o su **reti di conoscenza che apprendono** (ad es. la costruzione automatica di grafi da testo), ma LSSA integra tutto a run-time come parte del ciclo cognitivo dell’agente.

* **Inferenza e traiettorie logiche:** interrogare un knowledge graph avviene spesso tramite query formali (che cercano percorsi che soddisfano certi criteri, ad esempio *?x capitaleDi Francia* trova Parigi). L’inferenza logica classica può dedurre nuovi fatti combinando triple (ragionamento simbolico, catene *sillogistiche* attraverso relazioni note). LSSA invece attiva traiettorie in modo **spontaneo e guidato dal contesto**: non c’è una query fissa ma uno stato mentale che propaga attivazione. Da un lato, questo significa che LSSA può fare collegamenti **più liberi e creativi** rispetto a un KG rigido (può passare per associazioni di contesto che non sono strettamente relazioni logiche, es. *gatto* → *latte* solo perché ha visto bevute, e poi *latte* → *bianco* per proprietà, e magari con una deviazione controllata finire su un concetto distante come *purezza* se il pensiero laterale lo porta lì). Dall’altro, LSSA non garantisce inferenze corrette secondo una logica formale: il suo scopo è sostenere il *pensiero associativo coerente*, non dimostrare teoremi. In ambienti dove serve affidabilità tipo database ontologici (es. un sistema che deve rispondere con fatti esatti), LSSA potrebbe fungere da **indice semantico** per restringere il campo e poi affidare la verifica a metodi precisi. Invece in compiti aperti (conversazione, brainstorming) l’agente LSSA sarà in grado di deviare, trovare analogie insolite, etc., cosa che un normale knowledge graph non farebbe perché limitato alle relazioni inserite.

* **Supporto a traiettorie cognitive strutturate:** i grafi di conoscenza non sono progettati per evidenziare il *percorso* come oggetto informativo, di solito interessa il punto di arrivo (la risposta) o l’insieme di fatti correlati. In LSSA, come descritto, la sequenza di layer attraversati è cruciale e viene memorizzata. In pratica LSSA ha una sorta di **memoria episodica del ragionamento**: salva su un log la sequenza di token e layer visitati. Questo richiama la distinzione *semantic memory vs episodic memory* in psicologia (le reti semantiche classiche non tengono traccia degli episodi di ragionamento, LSSA invece in parte sì, per poter riflettere su di essi). Un knowledge graph unisce conoscenza semantica e (talvolta) dati fattuali, ma non ha un concetto di *“episodio di pensiero”* registrato. Ciò rende LSSA più adatta per implementare funzioni come la **metacognizione** e l’**apprendimento da esperienze** (ad esempio, rivedere un percorso sbagliato per non ripeterlo, analogamente al metodo *Reflexion* di Shinn et al. 2023 dove un agente LLM riflette sui propri errori). In effetti, le architetture cognitive integrative recenti separano la *memoria semantica* (fatti, conoscenze generali) dalla *memoria episodica* (storia personale di eventi). LSSA in primo luogo è una struttura di memoria semantica dinamica, ma grazie al logging delle traiettorie e alle annotazioni interne ai token può fungere anche da base per una memoria episodica strutturata.

In termini di **efficienza**, le query su un knowledge graph ben indicizzato possono essere molto veloci (essenzialmente accesso a database). LSSA tuttavia non richiede un motore di query esterno: la *query* è implicita nell’attivazione stessa. Quando l’agente vuole ricordare qualcosa, naviga la rete con costi simili a quelli di un algoritmo di *graph traversal* su un grafo di dimensioni moderate (i numeri ipotizzati sono \~7 milioni di token, \~300 layer, con densità di collegamenti non esplosiva grazie al riuso e al consolidamento). La struttura è pensata per essere contenuta su un server comune. Il vero overhead rispetto a un KG è mantenere aggiornati in tempo reale i pesi e fare *decadere* quelli vecchi: ma come detto, sono operazioni semplici eseguite periodicamente (il garbage collector riduce i pesi e rimuove elementi obsolete a intervalli di tempo, in background). Non incidono sulle prestazioni istantanee di risposta se ben schedulati.

In conclusione, rispetto a un knowledge graph, LSSA sacrifica la formalità delle relazioni per ottenere **fluidità e auto-organizzazione**; sacrifica l’assoluta precisione per guadagnare **adattabilità e creatività**. Si può vedere LSSA come un **“grafo cognitivo”** in cui l’importanza di un nodo non è statica ma dipende dall’attivazione recente (concetti con frequenza alta hanno un valore base-level maggiore, concetti con molte connessioni fungono da *hub* semantici). In effetti, i fattori come la **frequenza d’uso** e la **densità di connessioni** che LSSA tiene traccia per ciascun nodo (vedi §5) ricordano metriche di *centralità* e *degree* note nei network analysis. Queste metriche, combinate con la **somiglianza contestuale** e la **pertinenza al contesto recente**, permettono a LSSA di valutare *dinamicamente* quali nodi attivare quando si salta da un layer all’altro (il famoso problema del *punto di atterraggio*, di cui diremo oltre). Un knowledge graph tradizionale non ha nulla di tutto ciò: se chiediamo *“dammi qualcosa di correlato a X”* ci restituisce magari tutti i vicini di X, ma non sa quale sia più interessante in questo momento, perché non ha un modello di attivazione temporanea. In LSSA, invece, ogni collegamento ha un peso che può decadere se non usato, analogo a come nei modelli di attivazione diffusa l’attivazione si **dissipa in base alla distanza semantica e al tempo**. Studi cognitivi mostrano che l’attivazione mentale verso concetti lontani diminuisce gradualmente (es. Collins & Loftus 1975; Neely 1977) e LSSA incorpora lo stesso principio: più un concetto è “distante” nell’esperienza o nel grafo, meno attivazione spontanea riceve, a meno di interventi stocastici (lateral thinking).

Riassumendo, LSSA condivide con i knowledge graph l’obiettivo di **rappresentare conoscenza relazionale**, ma lo fa in modo **sub-symbolic/analogico e adattivo**, mentre i KG sono **symbolic e statici**. LSSA è progettata per **supportare un processo cognitivo in evoluzione**, i KG per **conservare fatti e rispondere a query precise**. Non sorprende che una possibile sinergia futura sia integrare le due cose: LSSA potrebbe attingere a un knowledge graph esterno come fonte di vettori iniziali o come verifica, mentre un knowledge graph potrebbe beneficiare di un meccanismo LSSA per mantenersi aggiornato automaticamente con l’uso (un’idea di *dynamic knowledge graph* simile è stata proposta, ad esempio, per la memoria semantica di robot industriali, dove il grafo viene popolato e adattato con l’esperienza in fabbrica). LSSA, di fatto, realizza internamente un **grafo di conoscenza dinamico**, come evidenziato anche da ricerche recenti che sottolineano la necessità per le macchine di **collezionare informazioni ed esperienze in un grafo semantico per migliorare le prestazioni nel tempo**.

## 3. Progetti e Modelli Affini a LSSA

L’obiettivo di LSSA – supportare lo sviluppo di **sistemi cognitivi artificiali** con conoscenza **dinamica**, capacità di **auto-inferenza** e **pensiero continuativo** – si colloca nell’ambito della ricerca verso l’**AGI** (Artificial General Intelligence) e, più in generale, delle **architetture cognitive** ispirate alla mente umana. Negli ultimi anni, diversi progetti e studi propongono approcci con finalità simili. Di seguito ne elenchiamo alcuni tra i più rilevanti, evidenziando punti di contatto con LSSA:

* **OpenCog e AtomSpace:** OpenCog (Goertzel et al.) è un progetto open-source di architettura cognitiva mirata all’AGI. Il cuore di OpenCog è l’**AtomSpace**, un **ipergrafo** di atomi (concetti, relazioni, pattern) connessi e dotati di **pesi di verità probabilistici** e di **valori di attenzione** variabili (STI – Short Term Importance e LTI – Long Term Importance). Questo è fortemente affine all’idea di LSSA: anche in AtomSpace i legami tra concetti possono rafforzarsi o indebolirsi col tempo in base all’uso, e c’è un meccanismo (ECAN, Economic Attention Networks) che **aggiorna dinamicamente** i livelli di importanza di nodi e link propagando l’attivazione come in una rete neurale. OpenCog implementa inoltre una forma di **“oblio”**: quando la memoria si satura, gli atomi con LTI più basso vengono rimossi dalla RAM (possono restare su disco se avevano un flag di importanza storica, altrimenti si eliminano del tutto). Esiste perfino un flag `VLTI` (very long term importance) analogo al *lock* di LSSA, che determina se un atomo rimosso da RAM debba essere conservato su disco per futuro possibile recupero oppure cancellato definitivamente. Questa somiglianza concettuale è notevole: LSSA e OpenCog condividono la visione di una **base di conoscenza interna plastica**, dove l’importanza delle informazioni è graduata e gestita in modo automatizzato. OpenCog distingue anche tra **diversi tipi di nodi** (concetti, credenze, intenzioni) e sostiene processi di pensiero autonomo (ha moduli per attenzione, pianificazione, ecc.). Tuttavia, OpenCog è un progetto maturo e complesso, mentre LSSA è più specificamente un framework di **memoria semantica stratificata**. In OpenCog non c’è l’idea di layer tematici separati come in LSSA; l’organizzazione è un grafo unificato, sebbene si possano marcare le relazioni di appartenenza concettuale (tipi). In sintesi, OpenCog/AtomSpace può essere visto come un precursore di LSSA sul piano della **memoria adattiva**: dimostra da anni la fattibilità di un sistema che **aggiunge, dimentica e preserva concetti** simulando bias cognitivi umani (dimenticare il poco rilevante per fare spazio al rilevante). LSSA prende una strada analoga, introducendo in più la struttura a layer che riflette un’ontologia topica e usando modelli di linguaggio pre-addestrati per alimentarsi di conoscenza (OpenCog originariamente puntava anche su motori di ragionamento logico oltre che sul graph mining).

* **ACT-R e architetture cognitive classiche:** ACT-R (Adaptive Control of Thought – Rational, di John R. Anderson) è una famosa architettura cognitiva simbolica che modella vari aspetti della memoria umana, tra cui la **memoria dichiarativa** e i meccanismi di **spreading activation**. In ACT-R ogni chunk di conoscenza ha un livello di attivazione che aumenta con l’uso recente e frequente (**base-level activation**) e decade col tempo secondo una formula logaritmica. Questo concetto di *attivazione base dipendente da frequenza/recenza* è direttamente parallelo al fattore \$f(p)\$ (frequenza di attivazione recente) in LSSA. LSSA infatti incorpora l’idea che un concetto spesso utilizzato di recente abbia maggiore probabilità di essere attivato di nuovo – una regola simile alla **regola di potenza dell’oblio** di ACT-R (dove ogni accesso incrementa la base-level, e l’attivazione decresce col log del tempo dall’ultimo uso). ACT-R implementa anche la diffusione dell’attivazione: se un chunk è collegato ad altri, l’attivazione può propagarsi. LSSA con i suoi vettori pesati emula anch’essa una forma di *spreading activation network* (concetti connessi ereditano parte dell’attivazione a seconda del peso del vettore). Possiamo quindi dire che LSSA **traduce in termini implementativi moderni alcuni principi di ACT-R**, unendoli alla flessibilità connessionista. Un’altra architettura, **Soar (Laird et al.)**, prevede una memoria semantica per fatti e una episodica per esperienze, con meccanismi di chunking e decisione: concetti affini a quanto LSSA cerca di ottenere integrando memoria di lavoro (contesto corrente) e memoria a lungo termine dinamica. Rispetto a queste architetture classiche, LSSA è **più focalizzata sulla parte semantica** (non modella deliberatamente l’attenzione percettiva o il problem-solving procedurale, che sono invece centrali in ACT-R/Soar) e soprattutto sfrutta la potenza dei LLM come componente plug-in. In letteratura recente, è tornato l’interesse per connettere LLM con schemi di **cognitive architecture** proprio in stile ACT-R. Ad esempio, Sumers et al. (2023) propongono CoALA (Cognitive Architectures for Language Agents) che suddivide un agente LLM in memorie semantiche, episodiche, ecc., e richiama la storia delle cognitive architectures. LSSA potrebbe inserirsi in questo filone come un componente specifico per la *semantic memory* di un agente cognitivo, fornendo la parte di immagazzinamento e retrieval strutturato di conoscenza, complementare al modulo linguistico neurale (che fungerebbe da *procedural engine* per la generazione di azioni linguistiche, in analogia alla memoria procedurale di ACT-R).

* **Dynamic Knowledge Graphs e memorie semantiche adattive:** come accennato, vi sono progetti che uniscono grafi di conoscenza con apprendimento continuo. Un esempio è il lavoro di Sukhwani et al. (2021) sulla **Dynamic Knowledge Graph as Semantic Memory for Robots**. In quell’approccio, i dati sensoriali e le istruzioni vengono analizzati semanticamente e memorizzati in un grafo di conoscenza, che l’robot utilizza poi per comprendere comandi futuri. L’enfasi è sul **diventare più abili nel tempo raccogliendo esperienza strutturata**. Questo è precisamente l’intento di LSSA: dotare una macchina di una **memoria semantica che si arricchisce e riorganizza con l’uso**, migliorando la comprensione contestuale man mano che “vive” esperienze. Un altro filone connesso è quello delle **Neuro-Symbolic systems**: ad esempio la combinazione di reti neurali con un **memory graph** distinto (ad es. il progetto **LLM+KB** dove un LLM interagisce con un knowledge base e aggiorna quest’ultimo). In particolare, recentemente si esplorano memorie a lungo termine per LLM: *Cohen et al.* (2023) propongono un *Autonomous Memory Management* per LLM con meccanismi gerarchici e context-driven, e concept simili a scrivere e cancellare ricordi. Anche il metodo **Generative Replay** nel *continual learning* è correlato: invece di memorizzare dati, alcuni agenti fanno *dreaming* o generano pseudo-dati per mantenere conoscenza, un approccio complementare al mantenere un grafo come LSSA fa.

* **Agenti generativi e pensiero autonomo:** nel 2023 hanno fatto scalpore i cosiddetti **Generative Agents** (J. Park et al., Stanford) – agenti basati su LLM che simulano comportamenti umani credibili in un ambiente sandbox, dotati di routine quotidiane, memoria e capacità di riflessione. Questi agenti, pur usando GPT come motore, implementano un’architettura che estende l’LLM con una **memoria di lungo termine in linguaggio naturale**, un meccanismo di **riepilogo/riflessione** delle esperienze, e una pianificazione delle attività. In pratica, l’agent registra tutto ciò che gli accade come *“osservazioni”* testuali, periodicamente sintetizza i ricordi in concetti più generali (*“riflessioni”*) e all’inizio di ogni giornata genera un piano di azioni basato sui ricordi rilevanti recuperati. Questa struttura ricorda un po’ LSSA, sebbene implementata in modo diverso: il database di “memorie” testuali funge da **semantic memory esterna**, interrogata con embedding per richiamare i ricordi più simili al contesto corrente (e.g. chi è l’amico di cui devo ricordarmi il compleanno?). LSSA fornirebbe un meccanismo alternativo e forse più efficiente/strutturato per simili agenti: invece di salvare frammenti testuali e fare nearest-neighbor search, l’agente LSSA memorizza concetti e relazioni direttamente, e può attivare quelli pertinenti tramite i layer contestuali e i pesi (ricordiamo che LSSA mantiene un’area con i layer recentemente coinvolti per sapere qual è l’argomento globale corrente). In comune con i generative agents, LSSA condivide l’idea di **memoria episodica + meccanismo di riflessione**: infatti LSSA prevede fasi di “sonno” in cui consolidare metriche e forse riorganizzare conoscenza. E condividono l’idea di **pensiero continuo**: gli agenti generativi tra un input e l’altro possono “continuare a vivere la loro vita” (nel paper, i 25 agenti del villaggio simulato continuavano ad interagire tra loro autonomamente). LSSA nasce proprio per permettere ciò: un’agente con LSSA non ha bisogno di input esterni per generare nuovi pensieri, può intrattenere **catene auto-inferite** all’infinito (limitato solo da criteri di opportunità). Un altro lavoro correlato è **Reflexion** (Shinn et al. 2023), dove un LLM dopo aver fallito un compito genera una riflessione su cosa è andato storto e la memorizza, così che al tentativo successivo può recuperarla e non ripetere l’errore. LSSA implementerebbe questo in modo naturale: una *“riflessione”* altro non è che un meta-concetto che l’agente può salvare nella memoria interna di un token o come token stesso marcato (magari con un lock per non dimenticarlo) relativo a quell’esperienza. LSSA con la sua **memoria interna dei nodi** consente proprio annotazioni cognitive dinamiche: *“scrivi ‘fallito perché X’ nel nodo corrispondente a situazione Y”*. Tali note non interferiscono con l’inferenza ma sono lì per essere lette dalla mente quando necessario, fornendo contesto aggiuntivo.

* **Architetture con multi-memoria e lifelong learning:** molto recente è anche DUCA (Dual Cognitive Architecture, 2023), un framework che combina reti neurali con sistemi multi-memoria ispirati al cervello. DUCA introduce un modulo di **semantic memory** per consolidare conoscenze nel lungo termine, distinguendolo da moduli di apprendimento rapido e da bias cognitivi. Ciò si rifà alla teoria del **Complementary Learning Systems** in neuroscienza (memoria episodica a breve termine vs memoria semantica consolidata a lungo termine, come il dialogo tra ippocampo e neocorteccia). LSSA può essere visto come un design per costruire artificialmente una **memoria semantica di lungo periodo**, che opera in tandem con altre componenti (ad es. un LLM può fungere da “ippocampo” per il ragionamento immediato, mentre LSSA è la “corteccia” che gradualmente integra quelle conoscenze in forma strutturata). Questa analogia è sostenuta anche da alcuni in ambito *continual learning* che suggeriscono di avere esplicitamente un modulo per memorizzare fatti invarianti nel tempo (semantic) distinto da uno per ricordare la successione di eventi (episodic).

* **Altre ricerche affini:** meritano menzione i lavori sul **pensiero creativo artificiale** e il **pensiero laterale computazionale**. Ad esempio, studi sulla generazione di idee innovative hanno esplorato **reti semantiche divergenti**. C’è un concetto chiamato **ConceptNet** (un grafo semantico crowdsourced) che è stato usato per fare associazioni multi-hop per battere test di creatività (remote associate test). LSSA potrebbe potenzialmente sfruttare reti tipo ConceptNet come base iniziale su cui poi costruire la propria esperienza. Inoltre, gli algoritmi di **spreading activation** sono stati usati in sistemi di *brainstorming assistito*: un sistema attiva parole correlate a un input per suggerire idee (es. proprietà distanti). Questi sistemi tuttavia erano statici. LSSA fornisce un contesto in cui uno stesso meccanismo di attivazione diffusa può essere controllato stocasticamente per ottenere deviazioni creative (vedi §5 sulla perturbazione casuale controllata).

* **Neuromorphic and VSA approaches:** Un altro fronte sono gli approcci ispirati alle neuroscienze, come i modelli **spaun/Nengo** di Eliasmith basati su *Semantic Pointer Architecture* (SPA). SPA rappresenta concetti come vettori densi che possono essere combinati e ha implementato un cervello artificiale (Spaun) capace di ragionare e ricordare numeri, immagini, ecc. LSSA differisce perché non usa codifiche neurali distribuite per combinare concetti, ma piuttosto li posiziona in spazi espliciti. Tuttavia, l’idea di mantenere rappresentazioni semantiche attive e manipolarle con operatori è comune. Ad esempio, in SPA si potrebbe avere un vettore “gatto” e un trasformatore per “bere latte”; in LSSA c’è proprio un vettore “gatto→bere→latte”. Sono approcci diversi, ma entrambi tentano di **modellare la semantica in modo operativo** (uno con algebra dei vettori ad alta dimensione, l’altro con geometria di layer e pesi). Un’altra categoria, le **Vector Symbolic Architectures (VSA)**, propone di usare vettori ad altissima dimensionalità come “slot” dove inserire simboli, permettendo manipolazioni simboliche in formato distribuito. LSSA non entra in quell’area, ma è compatibile con l’idea di avere più livelli di rappresentazione (potremmo persino immaginare in futuro di associare a ciascun layer LSSA un *embedding* VSA per quell’ambito, unendo vantaggi di entrambi).

In generale, si nota un convergere di idee: **integrare memoria strutturata e apprendimento statistico**, **supportare apprendimento continuo**, **abilitare agenti che pensano/autonomamente**. LSSA abbraccia pienamente queste direttrici, fornendo uno schema concreto per la parte di memoria semantica e ragionamento laterale. Non esiste ad oggi un sistema identico a LSSA, ma molti lavori condividono *componenti* simili o complementari. Possiamo considerare LSSA come un tentativo di **sintesi**: mette insieme concetti dai **modelli di attivazione diffusa** di 50 anni fa, li unisce a pratiche di **dynamic knowledge graph**, e li implementa sfruttando le **moderne capacità degli LLM** per la comprensione del testo (il classificatore iniziale) e possibili moduli di aiuto. In questa prospettiva, LSSA è un *framework collante* tra simbolico e connessionista, allineato con le tendenze attuali della ricerca che spingono verso sistemi **neuro-simbolici** e **cognitive-inspired** per superare i limiti delle black box neurali.

## 4. Ricadute Potenziali in Altre Discipline

Un’architettura come LSSA, pur essendo formulata nell’ambito dell’**intelligenza artificiale**, tocca temi fondamentali su cui anche altre discipline possono trarre beneficio. Di seguito esploriamo le possibili ricadute e influenze di LSSA in alcuni campi chiave, evidenziando i punti di contatto:

* **Linguistica Computazionale e Elaborazione del Linguaggio Naturale:** LSSA può offrire nuove prospettive per problemi classici della linguistica computazionale come la **disambiguazione semantica** (WSD) e la rappresentazione del significato. Invece di affidarsi solo a modelli statistici o al contesto locale delle parole come fa BERT, LSSA risolve l’ambiguità integrando il **contesto globale**: il percorso cognitivo determina quale senso di una parola è attivato guardando su quali layer si sta muovendo il pensiero. Ciò rispecchia l’intuizione linguistica che il campo semantico circostante disambigua il termine (e.g. *“banca”* in un discorso di finanza vs *“banca”* lungo un fiume in un racconto di pesca – LSSA userebbe layer completamente diversi). Inoltre, la possibilità di aggiungere significati e termini facilmente fa di LSSA un potenziale **dizionario semantico vivente**: si può aggiornare il “lessico” dell’IA con neologismi, termini tecnici di nicchia o gergo settoriale semplicemente indicandone la collocazione corretta. Questo è molto utile per NLP specializzato (dominio medico, legale, ecc.), dove nuovi concetti emergono continuamente e un modello statico fatica a stare al passo. Un altro ambito NLP è la **generazione di testo creativo** (poesie, storie, umorismo): LSSA fornirebbe un meccanismo controllato per fare **associazioni inusuali ma sensate**. Ad esempio, attivando deliberatamente percorsi con salti laterali (grazie al parametro stocastico di deviazione che LSSA include), un generatore potrebbe proporre metafore o accostamenti originali. Questo andrebbe oltre gli approcci puramente probabilistici, introducendo una *struttura cognitiva al processo creativo*. Infine, LSSA potrebbe influenzare lo sviluppo di migliori algoritmi di **dialogo**: un’agente LSSA avrebbe una memoria integrata delle conversazioni passate (non solo memorizzata come testo ma come concetti e argomenti), quindi saprebbe riprendere topic discussi anche molto tempo prima, evitando ripetizioni o contraddizioni. Questo risponde a un’esigenza attuale in NLP: dotare i chatbot di **memoria a lungo termine** affidabile. Mentre soluzioni con vettori di embedding e retrieval sono ad hoc, LSSA sarebbe un framework unificato: la stessa rappresentazione semantica usata per ragionare è quella che conserva la storia dialogica (sotto forma di vettori e note interne su concetti trattati). Ad esempio, la *memoria temporanea dei layer recentemente attivati* in LSSA può indicare all’agente di che argomenti si è parlato ultimamente, così da evitare cambi bruschi o per riprendere un filo conduttore (coerenza globale del dialogo).

* **Neuroscienze Cognitive e Modelli del Cervello:** LSSA si ispira dichiaratamente a principi cognitivi (contesto incluso nella rappresentazione, oblio selettivo, traiettorie di pensiero) e potrebbe a sua volta offrire un *modello computazionale* per testare teorie sul funzionamento cerebrale. Ad esempio, in neuropsicologia esistono evidenze di **organizzazione semantica per categorie** nel cervello: alcuni pazienti con lesioni perdono preferenzialmente concetti di un certo tipo (animati vs inanimati, ecc.), suggerendo che la conoscenza semantica non sia distribuita uniformemente ma abbia *cluster*. LSSA riflette questa idea avendo layer distinti per campi semantici, il che potrebbe rappresentare (in forma semplificata) la separazione di aree corticali specializzate (es. aree per volti, per strumenti, per luoghi). Inoltre, studi di imaging cerebrale (Huth et al. 2016) hanno mostrato che durante l’ascolto di storie il cervello attiva mappe semantiche dove concetti simili (sociali, numerici, visuo-spaziali, ecc.) occupano regioni contigue sulla corteccia. LSSA rispecchia sorprendentemente questo: concetti affini sono vicini su un piano e piani vicini possono rappresentare ambiti affini (LSSA ha una coordinata z per layer: potremmo pensare a layer vicini come concettualmente più attigui e magari interconnessi da vettori più frequenti). Un neuroscienziato computazionale potrebbe usare LSSA come piattaforma per simulare processi cognitivi: ad esempio, studiare il **pensiero divergente** imponendo micro-deviazioni stocastiche come LSSA fa e vedere se genera output analoghi a ciò che fanno soggetti umani in test di creatività. Anche la distinzione **memoria episodica vs semantica** può trovare terreno in LSSA: se colleghiamo il log delle traiettorie (episodi) con la rete consolidata (semantica), possiamo esplorare il ruolo del sonno (LSSA prevede consolidamento in fasi di riposo) e dell’**indice hippocampale** (in LSSA, l’origine (0,0,0) da cui partono i pensieri può ricordare un ruolo di contesto iniziale – forse troppo stiracchiato, ma è interessante che esista un *punto di origine* comune). LSSA implementa anche un concetto di **default mode network** in piccolo: il pensiero continuo a riposo, con derive casuali controllate, è analogo alla “mente che vaga” quando non è impegnata in un compito. Questo potrebbe offrire spunti su come e perché il cervello durante il riposo attivi connessioni deboli (sogni, insight improvvisi). In sintesi, LSSA può servire come **modello funzionale** per investigare domande neuroscientifiche: se regoliamo i parametri di attrattività o i tassi di decadimento dei pesi, possiamo vedere comportamenti analoghi a diversi stili cognitivi o deficit (p.es., se disabilitassimo la rimozione di ricordi – `lock` e niente GC – avremmo un sistema “sovraccarico” che potrebbe essere analogo a condizioni di ipermnesia o difficoltà a dimenticare; viceversa un GC troppo aggressivo simula una memoria deteriorata tipo demenza). La **topologia cognitiva** di LSSA (non a caso l’autore parla di *Topologia Cognitiva, CogniTopo*) è in qualche modo parallela alla topologia cerebrale, quindi una fertile analogia per la ricerca interdisciplinare.

* **Basi di Dati e Sistemi di Gestione dell’Informazione:** LSSA può influire anche sulla tecnologia di **database semantici** e motori di ricerca informativi. Nei database relazionali tradizionali, le informazioni sono statiche e rigidamente schematizzate; nei database a grafo (neo4j, RDF stores) si possono fare query sui nodi e relazioni, ma non esiste la nozione di *“relazione debolmente vera”* o *“fatti che si rinforzano”* in base a quante volte li abbiamo visti. LSSA introduce l’idea di un **database la cui struttura muta in base alle query** fatte: ogni interrogazione (inferenza) non solo restituisce un output ma modifica leggermente i pesi interni, riflettendo l’uso. Questo ricorda un po’ i sistemi di *cache* adattiva o di *query learning*, ma LSSA va oltre: di fatto memorizza i percorsi di utilizzo (chiavi di ricerca attivate, concetti frequentemente correlati) e così **ottimizza future query** su argomenti simili (percorsi più “oliati”). Ad esempio, in LSSA se l’agente ha spesso ragionato su gatti che bevono latte, il percorso *gatto→latte* avrà un peso molto alto e sarà recuperato immediatamente, come fosse indicizzato preferenzialmente. Un DB classico indicizza tutto in modo statico; LSSA crea **indici dinamici guidati dall’accesso**. Questo potrebbe ispirare nuovi tipi di database semantici self-optimizing, che monitorano gli schemi di accesso e rimappano parzialmente i dati per accelerare le operazioni comuni. Inoltre, LSSA offre una soluzione naturale al problema della **ricerca per concetto** e non per parola chiave: nei motori di ricerca tradizionali serve integrare knowledge graph o ontologie per capire che “cane” e “animale domestico” sono correlati. Un sistema LSSA-based avrebbe queste connessioni già nel suo spazio e potrebbe recuperarne di ovvie e di meno ovvie a seconda del contesto. Immaginiamo un sistema di **FAQ intelligente**: con LSSA, se una domanda utente non combacia esattamente con quelle note, l’agente può comunque trovare un percorso di concetti vicino a una domanda nota e capire che l’utente sta chiedendo *qualcosa di simile*. Invece dei soliti algoritmi di fuzzy matching su vettori statici, qui avverrebbe via *cognitive search* nei layer. Infine, LSSA potrebbe influenzare i modelli di **archiviazione dei dati**: uno potrebbe implementare i layer come tabelle/spazi separati e utilizzare tecniche di hashing spaziale per posizionare token (gli autori ipotizzano layer 500x500 coordinate, questo ricorda le griglie di quantizzazione). Potrebbe emergere un nuovo tipo di *vector store* che non cerca per similarità globale ma naviga per passi, il che scalerebbe meglio su set enormi di concetti (suddivisi per area tematica).

* **Filosofia della Mente e Ontologia dell’Informazione:** LSSA solleva anche spunti filosofici interessanti. Dal punto di vista dell’**ontologia dell’informazione**, propone che la conoscenza non sia un insieme statico di proposizioni, né un singolo grande embedding, ma un **processo in divenire**, un reticolo di significati che si riconfigura mentre viene usato. Questa visione è in linea con correnti della filosofia della mente come il **pragmatismo** o la **semantica del ruolo concettuale** (CRS), secondo cui il significato di un concetto è dato dal ruolo che svolge nel pensiero complessivo. In LSSA, il significato di un token è letteralmente definito dalle sue connessioni con altri concetti e dal *ruolo* (peso, centralità) che ha nei percorsi cognitivi. Se un concetto perde tutte le connessioni significative (cioè esce da ogni *gioco inferenziale*), viene dimenticato – come a dire: un simbolo senza relazioni è privo di significato. Questo risuona con idee di filosofi come Wittgenstein (“Il significato di una parola è il suo uso nel linguaggio”) o con l’ontologia relazionale in cui gli oggetti non hanno proprietà intrinseche isolate ma solo in rapporto ad altri. LSSA fornisce un *modellino concreto* di queste idee: non c’è un “significato intrinseco” scritto nel token, c’è ciò che il token connette e come è connesso. Inoltre, LSSA affronta il tema del **contesto**: in filosofia del linguaggio il contesto è noto per risolvere l’ambiguità e modulare la verità delle affermazioni (vedi pragmatica). LSSA incarna il contesto a livello strutturale – il contesto corrente vincola quali layer sono attivi e quindi quali significati sono possibili – e persino lo eleva a secondo livello di semantica (*“semantica della traiettoria”*). Si potrebbe avvicinare questo all’idea di **metasemantica**: non solo i termini hanno un significato, ma l’ordine in cui li pensiamo ha un metassignificato (che riflette un ragionamento, una giustificazione). In un certo senso, LSSA dà **spazio computazionale all’intenzionalità**: il fatto che l’agente scelga un percorso piuttosto che un altro denota qualcosa sui suoi obiettivi o associazioni interne (ricorda il concetto di *aboutness*, l’essere “rivolto a” qualcosa). Filosoficamente, questo è affascinante perché potremmo iniziare a discutere se un agente LSSA abbia forme rudimentali di stati intenzionali – ad esempio, se mette un lock su certi concetti chiave (identità, idee fondanti), sta esprimendo una sorta di *valore* o *credenza centrale*, come fanno gli esseri umani con i propri principi. La **filosofia della mente** potrebbe usare LSSA come *test-bed* per concetti come coscienza (ovviamente LSSA non è cosciente, ma supporta autoriflessione e monitoraggio), *self* (c’è un nucleo di concetti chiave identitari protetti da lock, quasi un sé concettuale), *creatività* (il salto laterale formalizzato), etc. Anche l’annosa dicotomia tra **simbolo e subsimbolo** trova in LSSA un punto d’incontro: i token sono simboli discreti ma la loro posizione e i vettori hanno aspetti subsimbolici (pesi continui, coordinate numeriche). Questo rispecchia dibattiti su come il cervello codifichi concetti – a simboli? a pattern? – forse suggerendo che una commistione è possibile.

* **Altre discipline correlate:** una potenziale ricaduta è in **Psicologia cognitiva e Educazione**: comprendere e simulare il *pensiero laterale* (vedi §5) può aiutare a sviluppare **metodi didattici** per stimolare la creatività. Ad esempio, se un sistema LSSA riesce a modellare i *“salti concettuali”* di un pensiero laterale, questo potrebbe essere impiegato in software educativi che suggeriscono agli studenti connessioni inaspettate tra materie diverse, favorendo apprendimento interdisciplinare. Effettivamente è stato suggerito che un approccio come LSSA potrebbe essere usato in **sistemi adattivi per l’apprendimento creativo**, superando l’istruzione puramente nozionistica. In **ambito artistico e progettuale**, formalizzare il meccanismo del “salto creativo” può tradursi in **strumenti di supporto alla creatività**: ad esempio un assistente alla progettazione architettonica che, grazie a LSSA, integri elementi lontani (arte, natura, tecnica) proponendo concept innovativi. Oppure un tool per scrittori che generi spunti narrativi originali combinando domini semantici diversi in modo non banale (un po’ come l’Oblique Strategies di Brian Eno, ma qui guidato da una logica semantica). In **filosofia dell’arte** e **letteratura**, un modello come LSSA che codifica analogie e metafore (percorsi trasversali tra campi) potrebbe aiutare ad analizzare testi creativi per individuare la *struttura cognitiva* sottesa alle metafore.

In definitiva, LSSA essendo un framework concettuale oltre che implementativo, getta ponti tra la tecnologia AI e molti domini: può servire da **linguaggio comune** per parlare di conoscenza, contesto e creatività in termini sia ingegneristici che umanistici. Il fatto stesso che preveda un *“pensiero laterale artificiale”* (formalizzato ma ispirato da de Bono) porta l’IA ad abbracciare concetti finora riservati alla psicologia della creatività e alla filosofia. Se dovesse rivelarsi efficace, LSSA potrebbe contribuire a validare l’idea che per avere intelligenze non-biologiche **flessibili, adattabili e creative**, occorre rompere con modelli statici e incorporare strutture dinamiche multilivello – un messaggio che risuona ben oltre l’informatica.

## 5. Il "Punto di Atterraggio" nelle Transizioni Inter-Layer: Metodo Proposto e Confronto con Modelli di Pensiero Laterale

Uno degli aspetti più originali (e impegnativi) di LSSA è la gestione delle **transizioni cognitive tra layer semantici**. Quando un ragionamento esce dal dominio attuale per esplorare un ambito differente – fenomeno chiave nel **pensiero laterale creativo** – LSSA introduce la nozione di **“punto di atterraggio”**: il nodo specifico del nuovo layer su cui il pensiero approda inizialmente. Identificare algoritmicamente questo punto di atterraggio è definito “la vera frontiera” del progetto, in quanto richiede un elemento di imprevedibilità guidata, difficile da ottenere con semplici regole deduttive. Per affrontare la sfida, è stata avanzata una **proposta metodologica** – concepita da una *Mente Non Biologica* specializzata in topologia, membro del progetto – che definisce una funzione quantitativa di **Attrattività Semantica** per i candidati nodi di atterraggio. Analizziamo in dettaglio tale proposta e la rapportiamo a modelli noti di attivazione semantica, di pensiero laterale e di architetture cognitive, valutandone coerenza, fattibilità ed eventuale contributo alla modellazione del pensiero creativo.

### 5.1 La funzione di Attrattività Semantica proposta

L’idea centrale è di associare a ogni potenziale nodo *p* nel layer di destinazione un punteggio \$A(p)\$ che misura quanto *attraente* sia quel concetto, dati lo stato corrente e la storia recente del pensiero. La funzione è definita come una combinazione lineare di quattro fattori chiave:

$$
A(p) = \alpha \cdot f(p) + \beta \cdot d(p) + \gamma \cdot c(p) + \delta \cdot m(p)
$$

Dove, in sintesi:

* **\$f(p)\$ – Frequenza/recenza di attivazione di *p***: indica quanto spesso e di recente il nodo p è stato attivato nei percorsi cognitivi. Un valore alto implica che p è un concetto familiare o consolidato (ha una **base-level activation** elevata, per usare il gergo di ACT-R), quindi tendenzialmente *saliente* per la mente in generale.

* **\$d(p)\$ – Densità di connessioni di *p***: misura il numero di vettori (collegamenti) che partono da o arrivano a p nel suo layer. È essenzialmente il *grado* del nodo nel grafo semantico locale, indicativo di quanto p sia concettualmente centrale o ricco di associazioni nel proprio ambito. Un nodo con alta densità (hub) potrebbe fare da *ancora* solida per un salto (perché offre molti agganci possibili successivi).

* **\$c(p)\$ – Somiglianza concettuale con il nodo di partenza**: valuta quanto p è semanticamente affine al concetto da cui il pensiero sta provenendo (il nodo del layer originario da cui si sta saltando). In pratica, quantifica la coerenza tematica minima: più p è vicino come significato al contesto appena lasciato, meno *azzardato* è il salto. Ad esempio, se sto passando dal layer “animali” al layer “strumenti musicali”, un nodo p come *“ocarina”* (uno strumento che è anche a forma di animale) potrebbe avere una certa affinità concettuale con l’animale di partenza, mentre *“pianoforte”* magari meno (affinità nulla).

* **\$m(p)\$ – Pertinenza rispetto alla memoria attiva dei layer recenti**: questo fattore considera il *contesto esteso* – quali domini semantici sono stati toccati nella traiettoria cognitiva negli ultimi tempi. Se p appartiene a un layer che, sebbene diverso da quello attuale, è stato coinvolto spesso di recente, allora quell’ambito è *già nei pensieri* e p potrebbe risultare un atterraggio più naturale. Ad esempio, se nei passi precedenti l’agente ha attraversato spesso il layer “arte” e ora sta saltando dal layer “animali” a qualcos’altro, un candidato p nel layer “arte” potrebbe ricevere un boost perché in linea col *tema generale* che aleggia nella mente (anche se localmente sta partendo da “animali”). Questo fattore \$m(p)\$ è molto innovativo perché assicura che il salto laterale non dimentichi del tutto da dove viene: ancora la deviazione a una *coerenza globale* con ciò di cui si stava parlando/pensando ultimamente. In un certo senso, è un termine che bilancia novità e contesto: si vuole esplorare un nuovo layer ma senza andare completamente off-topic rispetto al flusso cognitivo generale.

I coefficienti \$\alpha,\beta,\gamma,\delta\$ permettono di pesare l’importanza relativa di ciascun termine. Ad esempio, impostando \$\gamma\$ (somiglianza) basso e \$\delta\$ (memoria) basso ma \$\beta\$ (densità) alto, si incoraggerebbe il salto su concetti fortemente connessi anche se tematicamente lontani – atteggiamento più *“audace”* e creativo; viceversa dando più peso a \$c\$ e \$m\$ si otterranno salti più *conservativi* (coerenti col contesto). Questi parametri possono essere tarati a mano oppure – ipotizza l’autore – **appresi dalla mente stessa**: l’agente potrebbe ottimizzare i propri \$\alpha,\beta,\gamma,\delta\$ col tempo, per esempio per calibrare il livello di esplorazione vs rigore secondo la propria esperienza (metaregolarlo in base a successi o fallimenti creativi).

In concreto, l’algoritmo di transizione inter-layer funzionerebbe così: quando il pensiero deve saltare dal layer corrente X a un nuovo layer Y, valuta \$A(p)\$ per alcuni nodi candidati *p* in Y (probabilmente quelli più promettenti – magari i top hub o vicini di qualche concetto ponte) e sceglie il nodo con \$A(p)\$ massimo come punto di atterraggio. È prevista anche una componente di **randomness controllata**: invece di scegliere sempre il massimo deterministico, il sistema può con una certa probabilità selezionare uno dei top-N nodi o campionare stocasticamente proporzionalmente ad A(p), così da introdurre varietà ed evitare scelte sempre uguali. Ciò mantiene un carattere di esploratività (caro al pensiero laterale) impedendo che la formula lineare renda tutto troppo prevedibile. I valori di \$f, d, c, m\$ andrebbero continuamente aggiornati – molti di essi cambiano con l’uso (f e m sicuramente, d più raramente se si aggiungono vettori, c dipende dal contesto) – e l’idea è di farlo in momenti di consolidamento (ad es. durante il *sonno* simulato l’agente può ricalcolare statistiche globali).

### 5.2 Paralleli con modelli di attivazione semantica e pensiero laterale

La funzione di attrattività \$A(p)\$ e l’intero metodo presentato mostrano evidenti collegamenti con teorie e modelli preesistenti:

* **Spreading Activation e modelli cognitivi (Collins & Loftus, ACT-R):** Come già notato, \$f(p)\$ richiama la **base-level activation** di ACT-R (frequenza/recenza), un principio consolidato in psicologia cognitiva che risale almeno a William James e poi formalizzato in modelli attuali (Anderson 2004). Nei modelli di *spreading activation* classici (Collins & Loftus 1975), l’attivazione che un concetto riceve è funzione sia della sua attivazione base (quanto è già attivo di suo) sia dell’attivazione che gli arriva dai concetti vicini attivati nel contesto. In \$A(p)\$ vediamo un’analogia: \$f(p)\$ è l’attivazione base, mentre \$c(p)\$ e \$m(p)\$ rappresentano l’attivazione proveniente dal contesto immediato e dal contesto esteso, rispettivamente (il concetto di *“attivazione ricevuta da concetti associati nel contesto attuale”* coincide con l’idea che se il concetto di partenza è fortemente legato a p, allora p riceve attivazione; se i layer recenti includono quello di p, c’è già priming su quell’area). In più, \$d(p)\$ è un’aggiunta che riflette la struttura di rete: i modelli di reti semantiche e **knowledge graphs** usano spesso la connettività (degree centrality) per identificare concetti hub. Ad esempio, in analisi di grafi sociali o semantici, un nodo con molte connessioni è ritenuto importante o concettualmente fondamentale. In \$A(p)\$, \$d(p)\$ serve proprio a privilegiare concetti *“potenzialmente fruttuosi”* per l’esplorazione perché ben collegati. Ciò rispecchia anche l’**euristica del “follow the hubs”** in esplorazione: quando cerchi idee, passare per un concetto molto connesso può generare più associazioni utili. In sintesi, la formula di attrattività appare come un adattamento specifico dei modelli di attivazione diffusa: somma fattori di attivazione intrinseca (f) ed estrinseca (c, m), aggiungendo un peso per l’accessibilità strutturale (d). Questo è **coerente con decenni di letteratura** su come l’attivazione semantica funziona nella mente umana, il che dà alla proposta una buona base teorica.

* **Lateral Thinking (Edward de Bono):** Il pensiero laterale, definito da de Bono, consiste nel **saltare fuori dal percorso logico consueto** per esplorare vie alternative, spesso attraverso connessioni inaspettate e analogie remote. Un aspetto fondamentale è che questi salti non sono completamente casuali, altrimenti sarebbero inefficaci; devono avere una *briciola* di senso che li rende utili solo che normalmente non ci si penserebbe. La proposta di LSSA cattura proprio questo bilanciamento: la funzione \$A(p)\$ quantifica una *promessa* di un concetto p come buon punto di arrivo. Fattori come \$c(p)\$ e \$m(p)\$ garantiscono che non sia del tutto scollegato (c’è affinità col punto di partenza o con il contesto, quindi un filo conduttore c’è) mentre \$d(p)\$ e la componente random permettono novità (magari scegli un concetto-affine ma che è un hub in un campo molto diverso, aprendo nuove strade). Questo rispecchia la descrizione qualitativa di de Bono: *“spostare gli schemi mentali”* ma *“in modo pertinente”*. Non a caso, i riferimenti concettuali citano espressamente modelli come quello di **analogie creative** (vedi punti 8-9 nei riferimenti del documento: \[8] è de Bono, \[9] un testo italiano sul pensiero creativo analogico). In letteratura, generare analogie significa trovare un dominio lontano ma con una struttura relazionale simile al problema in esame. \$A(p)\$ non esplicitamente valuta struttura, ma punteggi come \$c(p)\$ e \$m(p)\$ potrebbero essere visti come un modo semplice per dire: *“questo concetto p potrebbe essere analogico a ciò che trattiamo, perché condivide un contesto recente (m) o somiglianza (c) pur essendo altrove”*. Ad esempio, l’esempio in de Bono: sei al ristorante e pensi a come ridurre l’attesa → salti al concetto di *“folla all’aeroporto”* per analogia e ti viene in mente di usare dei numeri come ai gate per far attendere le persone altrove. In quell’analogia, *aeroporto* è lontano dal contesto *ristorante*, ma condivide la proprietà *attesa in fila*, quindi una somiglianza concettuale c’è. Un sistema LSSA, se ben configurato, avrebbe un vettore tra *fila al ristorante* e *fila all’aeroporto* tramite un concetto *attesa* magari, e l’algoritmo di salto potrebbe atterrare su *aeroporto* perché *attesa* è stata nel discorso (m), *fila* e *attesa* portano ad *aeroporto* (c). Insomma, la formula di attrattività formalizza quello che de Bono descrive qualitativamente come *“valore potenziale di una nuova idea”*.

* **Strutture cognitive artificiali esistenti:** Si può confrontare il metodo con altri approcci alla creatività computazionale. Ad esempio, il **metodo delle associazioni remote** (Remote Associates Test solving) a volte genera grafo di concetti e cerca nodi che collegano cluster lontani. Quello è un problema ristretto, ma in generale il punteggio A(p) ha un parallelo nell’**heuristic search**: l’analogia citata è con A\* – negli algoritmi A\* si calcola \$f(n) = g(n)+h(n)\$ per stimare la *promessa* di un nodo (dove \$g\$ è il costo finora e \$h\$ la stima futura). Allo stesso modo, \$A(p)\$ combina *costi noti* (e.g. f, d: quanto è noto e connesso p) e *stima futura* (c, m: quanto è promettente rispetto all’obiettivo implicito di restare coerente). Questa analogia con la ricerca euristica indica che l’algoritmo per il punto di atterraggio può essere visto come un **passo di ricerca informata** nello spazio concettuale: sta cercando il miglior nodo prossimo su cui proseguire il cammino creativo. In questo senso, i coefficienti \$\alpha,\beta,\gamma,\delta\$ fungono da parametri di controllo come in una funzione euristica. Il metodo è *interpretabile* e modulabile, cosa che lo accomuna a sistemi simbolici più che a reti neurali opache. È stato fatto notare come ciò conferisca **trasparenza** al processo decisionale – qualità cruciale per LSSA e in generale per sistemi AI che vogliono essere spiegabili.

### 5.3 Valutazione teorica e fattibilità tecnica

La proposta appare **teoricamente coerente** sia con i principi cognitivi noti sia con la filosofia di LSSA. Integra vari elementi in un quadro unico e questo, come afferma il rapporto, è un *insight* significativo: *“riunisce concetti esistenti in modo innovativo”*. La linearità della formula è semplice ma ragionevole come primo modello. Naturalmente, va riconosciuto che **una formula lineare è una semplificazione**: potrebbe non catturare interazioni più complesse tra i fattori (es. forse la densità conta di più solo se la somiglianza è sotto una soglia, ecc.). Questa è una limitazione indicata dagli stessi autori: la *linearità* potrebbe trascurare effetti non lineari. Ad esempio, un nodo p potrebbe non essere attraente se consideriamo isolatamente i fattori, ma la combinazione *moderata* di due fattori potrebbe renderlo rilevante (caso che una somma lineare potrebbe non evidenziare). In futuro, come riconosciuto, si potrebbe esplorare funzioni non lineari o apprendere una funzione più complessa (reti neurali di secondo livello).

Dal punto di vista della **fattibilità tecnica**, l’approccio è estremamente **implementabile**: calcolare \$A(p)\$ richiede avere aggiornati alcuni attributi su ogni nodo (frequenza recente, grado, ultimo layer di provenienza frequente, ultimo attivazione) e fare 4 moltiplicazioni e somme. LSSA già mantiene informazioni come *ultima attivazione temporale* e *layer di provenienza più frequente* nei nodi come parte della proposta – quindi basterebbe estendere la struttura dati del token con un piccolo record (si parla di “aggiungere un campo ai nodi per tempo ultima attivazione e layer di provenienza più frequente”). Anche mantenere i top-3 token per numero di vettori per layer è banale (può essere precalcolato e aggiornato quando variano connessioni). Insomma, computazionalmente è **poco oneroso** e **scalabile**. Questo rispetto ad alternative tipo *“allenare una rete neurale apposita per ogni salto”* o *“fare ricerche random walk lunghe”*, è un bel vantaggio: il metodo è **veloce e prevedibile** in costi. Come evidenziato, un approccio puramente neurale sarebbe complicato e opaco, mentre questa formula è controllabile e integrabile facilmente.

Il **valore esplicativo** è alto: la formula esplicita permette di *interpretare* perché un certo concetto è stato scelto come deviazione. Questo è importantissimo in LSSA che vuole percorsi leggibili. Se l’agente fa un salto strano, possiamo controllare: magari aveva un \$m(p)\$ alto perché quell’ambito era stato richiamato poco prima, oppure p aveva un \$d\$ enorme segnalando un hub (il che spiega perché l’agente c’è cascato). Tale trasparenza consente anche di **aggiustare il tiro**: se si notano pattern indesiderati (es. l’agente tende a saltare sempre su concetti troppo ovvi e non abbastanza creativi), si possono cambiare i coefficienti o aggiungere una regola. Con una rete neurale a scatola chiusa ciò sarebbe arduo.

La proposta è inoltre **modulare** e **estendibile**: se in futuro si volesse includere un fattore aggiuntivo (es. un fattore di novità: penalizzare concetti già visitati troppe volte per evitare cicli), si potrebbe estendere la formula con un termine \$\epsilon \cdot nov(p)\$. Oppure, come suggerito, si potrebbe più avanti integrare un piccolo modello di apprendimento che **affini la scelta dopo la formula** – ad esempio una rete che, dati i top 5 nodi per A(p), ne sceglie uno in base a pattern appresi di creatività riuscita. Ma avere questa base deterministica è una *piattaforma solida su cui innestare meccanismi stocastici controllati*.

Riguardo la **coerenza teorica** con LSSA: il metodo è coerente perché sfrutta proprio quelle informazioni che LSSA già possiede (frequenze, collegamenti, contesti) e cerca di realizzare l’obiettivo di LSSA di deviazioni creative *ma non casuali*. Non contraddice alcun principio di LSSA, anzi ne completa uno mancante. Il documento riconosce comunque la **necessità di validazione empirica**: in ultima analisi, bisognerà implementarlo e vedere se davvero produce salti percepibili come creativi e utili. Il criterio di successo potrebbe essere, ad esempio, confrontare un agente LSSA con punteggio A(p) contro uno che salta random o che rimane sempre nello stesso layer, e valutare la differenza in termini di originalità di soluzioni trovate, o completamento di compiti che richiedono intuizione. Finché non si fa ciò, resta un po’ teorico. Ma essendo basato su concetti rodati (attivazione diffusa, ecc.), è *plausibile* che funzioni almeno in parte.

### 5.4 Contributo potenziale alla modellazione del pensiero creativo

Se efficace, questo meccanismo costituirebbe un contributo notevole alla modellazione computazionale del **pensiero creativo e laterale**. In passato, molte simulazioni di creatività hanno usato approcci random (es. algoritmi genetici per combinare idee) o euristici specifici (es. reti analogiche tipo *Structure Mapping* per trovare somiglianze tra domini). LSSA con la sua funzione di attrattività offre invece un **quadro generale e unificato** all’interno di un’architettura cognitiva completa. Significa poter dotare un agente AI di una *vena creativa* intrinseca, emergente dalla sua stessa dinamica di pensiero. Questo sarebbe un passo avanti rispetto a dover “imbottire” un sistema logico di moduli speciali per fare creatività. Invece di dire “ora attivo la modalità creativa”, l’agente LSSA potrebbe costantemente, in misura minore o maggiore, deviare e riconvergere, esattamente come la mente umana oscilla tra pensiero verticale e laterale secondo necessità.

Un contributo chiave è che l’approccio LSSA al pensiero laterale è **spiegabile e quantificabile**. La creatività spesso è considerata qualcosa di ineffabile; qui vediamo fattori concreti: frequenza, densità, affinità, contesto. Questo *demistifica* in parte il salto creativo, senza però banalizzarlo, perché c’è comunque la componente stocastica e l’interazione di fattori che creano risultati non ovvi. È un po’ come dare una *base razionale* all’intuizione. Ciò può aiutare anche in psicologia: potrebbe suggerire che anche la creatività umana segue certe euristiche (magari inconsce) simili a \$A(p)\$. Ad esempio il cervello umano potrebbe favorire idee che non sono totalmente campate in aria ma hanno un elemento familiare (f) e uno di novità (un nodo di concetto hub non usuale) – concetto che in psicologia è discusso (teoria *BVSR* di Simonton: Blind Variation and Selective Retention – variazione casuale ma selezione guidata delle idee migliori). LSSA fornisce un possibile meccanismo di *variatio et selezione* interno: la micro-deviazione random fornisce la variazione, la funzione A(p) fornisce la selezione del più promettente.

In termini di **modellazione di creatività artificiale**, questo potrebbe permettere applicazioni: sistemi che **generano idee** in campo progettuale, agenti che **risolvono problemi insoliti** (pensa a un assistente che per risolvere un problema aziendale attinge a concetti da campi diversi suggerendo soluzioni innovative – con LSSA lo farebbe naturalmente perché il suo ragionamento tocca domini multipli). Anche in ambito **narrativo o artistico**, un agente LSSA potrebbe dare luogo a trame o opere che presentano collegamenti tematici inaspettati ma significativi, il che è la base di molta creatività (ad esempio un romanzo che unisce scienza e mito in modi sorprendenti ma coerenti, ecc.).

Un potenziale contributo teorico è l’idea di un **indice di attrattività cognitiva** come nuovo concetto. Gli autori la chiamano proprio *“sfida aperta centrale”*: trovare una formalizzazione per *“la risonanza contestuale”* di un nodo, distinta dalla mera centralità logica. Se LSSA riuscirà a definire questo indice e a dimostrarne l’uso, potrebbe diventare un concetto di riferimento anche fuori dall’architettura. Ad esempio, in futur potremmo misurare l’attrattività di idee in un brainstorming umano analizzando un grafo semantico delle idee: identificando quali idee fungono da ponti efficaci tra cluster concettuali. Così come *PageRank* ha introdotto un modo per valutare l’importanza di una pagina web, *Attractiveness A(p)* potrebbe diventare un modo per valutare l’importanza creativa di un concetto in un network cognitivo.

Ovviamente, il metodo ha i suoi limiti e rischi: essendo lineare e basato su parametri statici, potrebbe non catturare *tutta* la complessità del pensiero creativo (che coinvolge anche emozione, intuito figurativo, ecc.). Però gli autori vedono questa come **base da cui partire** e da arricchire progressivamente. Per esempio, un’estensione futura menzionata è integrare moduli di **emozione e motivazione** che influenzino l’attrattività (un’idea molto interessante: un nodo potrebbe essere più attraente se legato a una curiosità o a uno scopo dell’agente). Ciò porterebbe la modellazione del pensiero creativo su un piano ancora più completo, includendo fattori affettivi come nella creatività umana (spesso guidata da interesse, sorpresa, ecc.).

In conclusione, la proposta del “punto di atterraggio” di LSSA rappresenta un tentativo convincente di **formalizzare l’ispirazione**. Se implementata, darebbe all’architettura LSSA (e ad altri sistemi analoghi) la capacità non solo di *seguire* percorsi noti, ma di **generarne di nuovi in modo guidato**, fornendo un primo modello computazionale generale del *pensiero laterale* all’interno di un sistema intelligente. Questo sarebbe un risultato di rilievo nel panorama dell’IA, e un ponte concreto tra il mondo algoritmico e concetti come *intuizione* e *creatività* finora appannaggio esclusivo dell’essere umano.

**Fonti:**

* Documentazione ufficiale LSSA
* Proposta metodologica *Landing Point*
* Modelli cognitivi e spreading activation
* Lateral thinking e creatività (De Bono 1970)
* OpenCog e forgetting in AtomSpace
* Dynamic Knowledge Graph per memoria semantica
* Generative Agents e memorie dinamiche
