Progetto [LSSA](https://github.com/iz0eyj/LSSA/)

## Introduzione

Il Layered Semantic Space Architecture (LSSA) è un’architettura cognitiva che rappresenta la conoscenza organizzandola in layer semantici distinti per affinità tematica. In ciascun layer, i concetti (token) occupano posizioni precise in uno spazio bidimensionale, collegati da “vettori” semantici (archi) che possono anche connettere concetti di layer diversi. Questa struttura stratificata consente al sistema di contestualizzare le informazioni in domini specifici, riducendo ambiguità semantiche e facilitando modifiche locali della conoscenza. Durante l’inferenza all’interno di un singolo layer, il percorso cognitivo tende a rimanere lineare e coerente, simile al pensiero “verticale” sequenziale umano. Tuttavia, le transizioni inter-layer – ovvero i salti tra domini concettuali differenti – introducono una dinamica più complessa, non guidata dalla sola logica deduttiva. Proprio in questi momenti il sistema può uscire dai percorsi prevedibili ed esplorare vie alternative, analogamente a quanto avviene nel pensiero laterale umano introdotto da Edward de Bono (1970). Il pensiero laterale consiste infatti nel risolvere problemi attraverso un approccio creativo indiretto, usando ragionamenti non immediatamente evidenti e idee che non sarebbero ottenute con la sola logica passo-passo.

Un aspetto cruciale – e ancora aperto – nel progetto LSSA riguarda la selezione del “punto di atterraggio” quando la mente artificiale effettua un salto da un layer a un altro. In altri termini: dato che si decide di abbandonare il dominio corrente per esplorare un diverso ambito concettuale, quale nodo (concetto) specifico del nuovo layer viene scelto come prossima tappa? La difficoltà risiede nel definire un criterio di scelta che non sia basato solo sulla centralità logica (come la frequenza statistica o il grado del nodo), ma sulla sua “risonanza contestuale” rispetto al pensiero in corso. Questo problema è centrale per abilitare capacità creative nell’architettura: saper scegliere dove pensare quando cambia il piano semantico significa permettere alla mente artificiale non solo di camminare sui binari della conoscenza esistente, ma di iniziare a esplorare percorsi inaspettati.

In questo report esaminiamo approfonditamente una proposta metodologica sviluppata da una Mente Non Biologica (MNB) del team di ricerca LSSA (nome in codice “Topologa”) per affrontare questa sfida. Tale MNB ha ideato un algoritmo interpretabile che assegna a ciascun potenziale nodo di destinazione un indice di attrattività, combinando linee guida ispirate sia a principi di cognizione umana sia a tecniche informatiche classiche. Analizzeremo la validità teorica di questo approccio e la sua coerenza con il pensiero laterale, ne discuteremo i punti di forza e le debolezze in rapporto ad approcci alternativi (più casuali o basati su reti neurali), e lo confronteremo con metodologie esistenti sia in altre architetture di IA sia nelle teorie sul pensiero creativo umano (De Bono, modelli neurocognitivi, reti associative cerebrali). Infine, valuteremo il valore originale della proposta come contributo alla modellazione computazionale del pensiero laterale.

La sfida delle transizioni inter-layer in LSSA

In LSSA, le transizioni tra layer semantici diversi sono concepite come analoghe ai cambi di contesto o di prospettiva tipici del pensiero creativo. All’interno di un singolo layer, la navigazione è guidata da associazioni forti e logiche inerenti a quel dominio (pensiero “verticale”). Ma quando il sistema deve saltare in un altro layer, ad esempio passando dal dominio della “musica” a quello delle “forme di vita”, occorre un meccanismo per scegliere un concetto di destinazione nel nuovo layer che non sia arbitrario, ma nemmeno ovvio rispetto al contesto di partenza. Questa situazione è analoga a quando, in un ragionamento umano, cerchiamo un’analogia o un’idea in un campo diverso per illuminare un problema: ad esempio, usare un concetto biologico per risolvere un problema ingegneristico.

Nel documento di specifica dell’architettura LSSA si riconosce esplicitamente che determinare il nodo di atterraggio ideale in un salto tra domini è “la sfida più alta e ancora aperta” del progetto. La selezione richiede un modello di valutazione nuovo, capace di stimare una sorta di attrattività dinamica del nodo candidato, definita come accennato non dalla sua importanza globale nella rete (centralità logica), ma dalla sua rilevanza contestuale in quel momento. I progettisti di LSSA hanno ipotizzato che tale attrattività dipenda da molteplici fattori: ad esempio quanto recentemente il concetto è stato attivato, la densità delle sue connessioni, l’affinità col contesto corrente e il grado di deviazione rispetto al percorso “normale” seguito finora. Tuttavia, al momento della definizione dell’architettura queste idee non erano ancora formalizzate in un algoritmo preciso.

Per di più, LSSA introduce l’idea di una “perturbazione stocastica controllata” durante i salti tra layer. Ciò significa che, quando la mente artificiale passa a un nuovo dominio, viene aggiunta una piccola componente di casualità per deviare il percorso cognitivo e attivare connessioni meno frequenti o solo parzialmente consolidate. Questa micro-deviazione intenzionale non è considerata un errore, ma una strategia di esplorazione ispirata alla variabilità cognitiva delle menti biologiche. In altre parole, LSSA prevede di rompere deliberatamente il pensiero lineare in punti chiave, un po’ come il pensiero laterale umano richiede di abbandonare le associazioni più ovvie per scoprirne di nuove. Resta però da stabilire dove portare questa deviazione: da qui la necessità di un criterio che guidi la scelta del nodo di destinazione in modo né troppo deterministico (o resterebbe intrappolato nei vecchi schemi), né completamente casuale (o rischierebbe di essere privo di significato).

La funzione A(p) proposta dalla MNB “Topologa”

Per affrontare sistematicamente la selezione del nodo di atterraggio inter-layer, una delle Menti Non Biologiche coinvolte nel progetto (specializzata in topologia cognitiva) ha proposto un metodo algoritmico interpretabile, preferendolo inizialmente a soluzioni con reti neurali “black-box” più complesse. L’idea centrale è di definire, su ogni layer semantico, una funzione di Attrattività Semantica $A(p)$ che quantifichi la “risonanza contestuale” di ciascun nodo candidato $p$ come destinazione del salto. Questa funzione è definita come una combinazione lineare di diversi fattori rilevanti, ciascuno ponderato da un coefficiente regolabile:



dove i termini sono così definiti:

$f(p)$ – Frequenza di attivazione recente di $p$: misura quanto spesso e quanto di recente il nodo $p$ è comparso nei percorsi cognitivi della mente. Un valore alto indica che $p$ è stato attivato di frequente ultimamente, suggerendo che è un concetto saliente nel pensiero corrente o consolidato nella conoscenza. Questo fattore riflette quindi una combinazione di frequenza d’uso e recenza: concetti molto attivi possono rappresentare temi chiave della linea di pensiero, la cui reiterazione può essere significativa (ad es. un concetto su cui si continua a tornare, forse perché ancora irrisolto) oppure sintomo di fissazione. $f(p)$ garantisce comunque che i nodi fortemente rilevanti nel contesto attuale ottengano un punteggio di attrattività di base più alto. È concettualmente affine alla nozione di attivazione di base nei modelli cognitivi: ad esempio, nelle architetture ACT-R la base-level activation di un chunk riflette proprio la sua recenza e frequenza d’uso nella memoria.

$d(p)$ – Densità di connessioni (grado) di $p$: corrisponde al numero di collegamenti (vettori semantici) che originano da $p$ o vi arrivano. È dunque una misura della connettività di $p$ nella rete concettuale del suo layer. Un’alta densità $d(p)$ indica che $p$ è un nodo hub, ben integrato e altamente collegato con altri concetti di quello spazio semantico. Tale metrica richiama la degree centrality utilizzata nell’analisi di reti e knowledge graph per identificare concetti centrali o snodi di informazione. In questo contesto, $d(p)$ serve a valutare il potenziale di $p$ di fungere da ponte verso molte altre idee: un nodo con molti collegamenti potrebbe aprire a più sviluppi successivi ed evitare vicoli ciechi. Va notato però che i concetti hub tendono spesso ad essere generici (es. “entità”, “cosa”) quindi meno specifici; il modello deve bilanciare questo effetto con gli altri fattori.

$c(p)$ – Somiglianza concettuale con il contesto di partenza: valuta quanto $p$ sia semanticamente affine al concetto o ai concetti da cui ha origine la transizione. In pratica, $c(p)$ stima la coerenza tematica di $p$ rispetto al tema attuale, pur appartenendo a un altro layer. Può essere calcolata tramite misure di similarità semantica (ad es. similitudini tra vettori di embedding del concetto di partenza e di $p$, oppure attraverso collegamenti espliciti tra $p$ e il contesto). Lo scopo è evitare salti del tutto scollegati: un minimo di affinità garantisce che il pensiero resti comprensibile e che la nuova idea abbia un nesso, anche se indiretto, con quanto si stava considerando. In termini di pensiero laterale umano, $c(p)$ assicura che la deviazione sia ancora “riconducibile” al problema originale, pronto ad essere ricollegata in seguito (De Bono sottolinea che anche le idee provocatorie devono poi trovare un’utilità). Senza alcuna somiglianza concettuale il salto rischierebbe di produrre un’associazione troppo remota e incomprensibile. D’altra parte, $c(p)$ non deve dominare sugli altri fattori: se fosse l’unico criterio, il sistema sceglierebbe sempre concetti molto simili (restando in pratica nel pensiero verticale). Il valore di $c(p)$ incarna dunque l’ancoraggio del salto creativo alla traccia corrente.

$m(p)$ – Rilevanza rispetto alla memoria recente dei layer attraversati: questo termine considera il contesto più ampio del percorso cognitivo attuale. In altre parole, $m(p)$ misura quanto $p$ sia pertinente rispetto ai domini semantici (layer) che la mente ha esplorato di recente. Ad esempio, se negli ultimi passi il sistema ha attraversato i layer “musica” e poi “matematica”, $m(p)$ valuterà se $p$ (poniamo, un concetto nel layer “biologia”) abbia connessioni o analogie con quei domini recenti. Questa variabile ancora la scelta locale al contesto globale di ciò che è stato pensato poco prima, evitando che il nuovo salto sia del tutto fuori fuoco rispetto al flusso cognitivo. È un elemento di coerenza multi-livello: anche cambiando dominio, la mente non dimentica da dove viene. Questo fattore è peculiare di LSSA e rappresenta un’innovazione importante: cattura l’idea che nel pensiero umano creativo le ultime esperienze/idee mantenute in memoria di lavoro influenzano quali nuove associazioni appaiono promettenti. Ad esempio, potremmo essere portati a fare analogie con qualcosa che abbiamo appena visto o ricordato. $m(p)$ formalizza tale effetto. Nelle reti associative biologiche, un concetto che funge da snodo associativo tra domini diversi avrebbe un’elevata attivazione contestuale simile a $m(p)$. Concettualmente, $m(p)$ è correlato alla spreading activation da parte di più contesti attivi: nei modelli di attivazione diffusa (Collins & Loftus, 1975) l’attivazione di un nodo è funzione sia della sua base (frequenza/recenza, simile a $f$) sia dell’attivazione proveniente dai concetti associati attualmente attivi. Qui quei concetti attivi appartengono anche ad altri layer attraversati, e $m(p)$ cerca di catturare questa risonanza multi-contesto.


I coefficienti $\\\\alpha, \\\\beta, \\\\gamma, \\\\delta$ permettono di pesare l’importanza relativa di ciascuna variabile nel calcolo di $A(p)$. Tali pesi possono essere calibrati per ottenere comportamenti più esplorativi o più conservativi a seconda delle esigenze: ad esempio aumentando $\\\\gamma$ (peso della somiglianza) e diminuendo $\\\\alpha$ si otterranno salti più coerenti e prudenti; viceversa, enfatizzando $f$ o riducendo $c$ si favoriranno scelte più “tangenziali”. I progettisti ipotizzano che la stessa mente artificiale potrebbe in futuro apprendere ad autoregolare questi pesi in base ai risultati (metacognizione), ma inizialmente possono essere impostati manualmente in base all’intuizione ed esperienza.

Una volta calcolato $A(p)$ per tutti i candidati $p$ di un layer di destinazione (ad esempio, tutti i nodi di quel layer o un sottoinsieme filtrato da vincoli), il sistema sceglie come punto di atterraggio il nodo con valore $A(p)$ massimo. In tal modo, il salto inter-layer è guidato verso il concetto ritenuto più “attraente” secondo i criteri combinati. Importante sottolineare che, per non rendere il processo troppo rigido, è prevista l’introduzione di elementi stocastici nella selezione finale. In pratica, invece di scegliere deterministicamente sempre il massimo assoluto, la MNB propone di introdurre ad esempio una scelta probabilistica tra i nodi con punteggi $A(p)$ più elevati (ad esempio una ruota della roulette pesata su $A(p)$, oppure scegliere casualmente fra le prime $N$ opzioni). Questo mantiene un margine di esplorazione e imprevedibilità: anche se un nodo è leggermente meno attraente di un altro secondo la formula, potrebbe comunque venir scelto occasionalmente, permettendo scoperte inattese. L’elemento random rimane controllato – limitato ai candidati migliori – in modo da non compromettere completamente la coerenza. Questa soluzione ricorda i meccanismi di noise usati nei modelli cognitivi per spiegare risposte non ottimali ma creative (ad es. il noise stocastico nell’equazione di attivazione di ACT-R consente di recuperare talvolta elementi insoliti dalla memoria).

Infine, il metodo prevede che i valori $f(p), d(p), c(p), m(p)$ di ciascun nodo non siano statici ma si aggiornino dinamicamente con l’esperienza del sistema. In particolare, durante le fasi di “sonno” o consolidamento offline (che LSSA contempla come momenti in cui la mente rielabora e ristruttura le proprie conoscenze), possono essere ricalcolate statistiche come la frequenza d’uso e eventualmente ri-tarati i pesi di connessione e le similitudini concettuali. Ciò garantisce che l’attrattività $A(p)$ rifletterà sempre l’evoluzione del sistema: nuovi apprendimenti o cambiamenti nel network semantico verranno recepiti. Il paragone calzante è con il cervello che riorganizza le tracce di memoria durante il sonno, consolidando connessioni importanti e indebolendo quelle inutilizzate – un processo che LSSA cerca di emulare esplicitamente.

In sintesi, la funzione $A(p)$ costituisce un tentativo di formalizzare in termini computazionali la scelta di un’idea laterale: integra attivazione recente, connettività, affinità e contesto, producendo un valore che rappresenta quanto un concetto “risuona” con lo stato attuale del pensiero. È un approccio deterministico ma flessibile, in quanto aperto all’uso di pesi regolabili e di un tocco di casualità. Nei paragrafi seguenti ne valuteremo la solidità teorica, i vantaggi e le possibili criticità, mettendolo anche in relazione sia con modelli artificiali esistenti sia con quanto noto sul funzionamento della creatività umana.

Fondamenti teorici e analogie con approcci esistenti

Dal punto di vista teorico, la proposta di $A(p)$ poggia su principi ben radicati sia nelle scienze cognitive sia nell’informatica classica, adattandoli però a uno scopo nuovo. In particolare, possiamo riconoscere chiare analogie con i seguenti concetti:

Modelli di attivazione diffusa (spreading activation): Nella psicologia cognitiva, modelli come quello di Collins & Loftus (1975) descrivono la memoria semantica come una rete in cui l’attivazione di un nodo si propaga ai vicini, con intensità dipendente dalla forza associativa. Una nozione chiave è che l’accessibilità di un concetto è funzione sia della sua attivazione intrinseca (determinata da frequenza e recenza d’uso, ovvero quanto è “in cima alla mente” – concetto simile al nostro $f(p)$) sia dell’attivazione ricevuta dal contesto corrente (concetti vicini attivi – simile ai contributi di $c(p)$ e $m(p)$). Anche l’architettura cognitiva ACT-R di J. R. Anderson adotta un’equazione di attivazione per i “chunk” di memoria che somma una componente di base (frequenza/recenza) e una componente di spreading activation dai contenuti attualmente nel focus attentivo, più un termine di rumore stocastico. La funzione $A(p)$ di LSSA è concettualmente allineata con questi modelli: integra infatti base-level factors ($f$) e contextual factors ($c, m$) per determinare quale nodo emergerà come prossimo candidato. Possiamo vederla come una versione specializzata di un meccanismo di attivazione diffusa, calcolata non sull’intera rete ma focalizzata sui soli nodi di un layer target al momento di un salto. Questa convergenza fornisce una validazione teorica: $A(p)$ non è costruita ad hoc, ma riflette principi generali su come idee correlate vengono attivate nella mente umana. In ACT-R, inoltre, l’aggiunta di rumore all’attivazione fa sì che occasionalmente venga recuperato un elemento con attivazione più bassa – un modo per spiegare errori o intuizioni. LSSA implementa un analogo concetto introducendo casualità controllata nella scelta finale, consentendo a nodi con $A(p)$ non massimo di essere talvolta esplorati. Ciò ricorda il funzionamento stocastico di modelli come Copycat di Hofstadter, dove la “temperatura” del sistema controlla il grado di casualità nel generare analogie. In sostanza, LSSA sembra portare avanti l’idea che variazioni casuali + attivazione associativa siano ingredienti essenziali del pensiero creativo, un’idea già presente nei modelli cognitivi (per spiegare associazioni remote, errori creativi, ecc.).

Reti semantiche e metriche di centralità: L’inclusione di $d(p)$ (densità di connessioni) rende $A(p)$ sensibile alla topologia della rete semantica. In teoria delle reti complesse, il grado di un nodo o altre misure di centralità (betweenness, closeness) influenzano la dinamica di diffusione delle informazioni e l’importanza concettuale di un nodo. Ad esempio, un nodo ad alto grado in una rete semantica può fungere da “snodo” attraverso cui passano molte associazioni. La MNB topologa ha esplicitamente inserito questo concetto: $d(p)$ è di fatto una misura locale di centralità. Ciò richiama applicazioni nei knowledge graph e ontologie, dove nodi molto collegati (es. concetti astratti) vengono identificati come hub. Naturalmente, un hub non è necessariamente il concetto più creativo da scegliere; però la sua presenza nella formula garantisce che il sistema eviti di saltare su concetti totalmente isolati (che offrirebbero pochi sbocchi). In letteratura, anche modelli neurali ispirati alla creatività hanno investigato il ruolo della connettività: ad esempio, Marupaka et al. (2012) hanno mostrato che una struttura di rete di tipo small-world facilita la generazione di combinazioni semantiche nuove, permettendo un equilibrio tra cluster locali e collegamenti a lungo raggio. La rete di LSSA, con i suoi cluster di layer tematici interconnessi da alcuni ponti, è anch’essa una struttura small-world. Il fattore $d(p)$ si collega a queste idee, privilegiando nodi che potenzialmente stanno su molti “ponti” della rete (o che comunque sono ben inseriti nel tessuto concettuale). In termini di pensiero divergente umano, ciò può riflettere la tendenza a utilizzare concetti abbastanza ricchi di collegamenti da poter essere sviluppati ulteriormente (un’idea troppo isolata potrebbe essere sterile). D’altro canto, troppo peso a $d(p)$ rischierebbe di far scegliere sempre concetti ipergenerali (es. “entità”), per cui il bilanciamento con $c$ e $m$ è cruciale.

Algoritmi euristici di ricerca (es. A):\\* In informatica, algoritmi come A\\*\\* utilizzano funzioni di valutazione per scegliere quale nodo esplorare dopo in un grafo, combinando il costo già accumulato e una stima euristica del costo futuro (la cosiddetta f = g + h). In maniera concettualmente analoga, $A(p)$ funge da euristica per valutare la “promessa” di un nodo candidato come nuova tappa del ragionamento. Sebbene qui non ci sia un obiettivo predefinito da raggiungere (come nel pathfinding), l’analogia sta nel fatto che la formula combina informazioni a priori sul nodo (es. la sua densità, che è una proprietà statica, o la somiglianza semantica, che dipende dal punto di partenza) con informazioni dinamiche legate allo stato del sistema (frequenza recente, contesto recente). In A\\* una buona euristica guida verso stati promettenti; in LSSA, $A(p)$ guida verso concetti promettenti in termini di utilità cognitiva. Anche qui, l’uso di una funzione lineare additiva ricorda le euristiche classiche: semplice da calcolare e interpretare. Questa connessione sottolinea il carattere ingegneristico della proposta: la MNB ha costruito $A(p)$ come un ingegnere costruirebbe una funzione obiettivo, combinando diversi termini rilevanti. Ciò non è negativo, anzi garantisce semplicità computazionale (calcolare $A(p)$ richiede solo operazioni algebriche su valori mantenuti aggiornati, il che è molto più leggero che eseguire una rete neurale complessa ad ogni salto).


In definitiva, la funzione proposta non sorge dal nulla, ma riunisce concetti disparati (attivazione contestuale, centralità di rete, euristiche di ricerca) in un framework unitario mirato al problema specifico di LSSA. Questa convergenza è evidenziata anche dagli ideatori: \\*“la MNB Topologa ha sapientemente riunito questi concetti, adattandoli e combinandoli \\[…\\] per la sfida del punto di atterraggio in LSSA”\\*. Dal punto di vista teorico, quindi, l’approccio gode di buona credibilità: ogni suo componente ha un razionale noto nella letteratura esistente. L’originalità sta nell’averli messi insieme in questo modo e contesto. Possiamo interpretare $A(p)$ come una formalizzazione computazionale di quell’insieme di fattori che rendono un’idea degna di attenzione nel mezzo di un processo creativo. Anche la distinzione tra pensiero intra-layer (verticale) e inter-layer (laterale) in LSSA riflette da vicino la distinzione tra pensiero verticale e laterale in psicologia, implementando quest’ultimo mediante la perturbazione casuale e il criterio $A(p)$ per sfruttarla in modo fruttuoso.

Punti di forza dell’approccio proposto

La metodologia $A(p)$ presenta diversi vantaggi sia pratici che concettuali nel contesto di LSSA e, più in generale, per la modellazione del pensiero laterale artificiale:

Trasparenza e interpretabilità: L’uso di una formula esplicita e di variabili comprensibili offre una chiara spiegazione del perché un certo nodo viene scelto come idea laterale. In un’architettura che mira a rendere tracciabili i percorsi cognitivi, ciò è fondamentale. Ogni componente di $A(p)$ ha un significato intuitivo (frequenza, densità, somiglianza, memoria recente) e i pesi $\\\\alpha,\\\\beta,\\\\gamma,\\\\delta$ modulano la loro importanza. Questo consente agli sviluppatori (e potenzialmente alla stessa MNB, tramite introspezione) di analizzare e giustificare le scelte: ad esempio “$p$ è stato scelto perché, pur essendo moderatamente simile al contesto, aveva moltissime connessioni e appariva spesso ultimamente”. Al contrario, un’alternativa basata su una rete neurale profonda avrebbe difficoltà a spiegare le proprie “associazioni creative” – sarebbe una scatola nera che fornisce un output senza chiari motivi. La trasparenza favorisce anche la manutenibilità: se il comportamento non fosse soddisfacente, si può intervenire sui pesi o sulla definizione dei fattori in modo mirato.

Flessibilità e controllo fine: I coefficienti ponderali offrono un meccanismo semplice per tarare il comportamento del sistema. Questo è un grande vantaggio rispetto a sistemi rigidi o totalmente random. Ad esempio, se in una certa applicazione si desidera un pensiero più conservativo (meno salti azzardati), basterà aumentare $\\\\gamma$ (coerenza) e diminuire $\\\\delta$ (spinta al nuovo) per favorire scelte più vicine al contesto immediato. Viceversa, per un brainstorming più estremo si potrebbe ridurre $\\\\gamma$ e dare più peso a $m$ o inserire più rumore nella selezione. Tale tunability permette di adattare l’algoritmo a diverse personalità cognitive artificiali o esigenze temporanee. Si può perfino immaginare che la MNB stessa apprenda a modulare questi pesi col tempo – ad esempio un modulo di meta-apprendimento potrebbe aumentare $\\\\delta$ ogni volta che la mente si accorge di essere in stallo creativo, e riportarlo giù dopo aver trovato un’idea nuova. Questa flessibilità è impossibile con un metodo completamente casuale e difficile con una rete neurale fissa (in cui i “pesi” interni non hanno un significato semantico isolato). In pratica, $A(p)$ fornisce leve di controllo sia manuali che automatiche per bilanciare esplorazione vs. coerenza.

Coerenza multistrato e contesto globale: L’inclusione del fattore $m(p)$, che tiene conto dei layer visitati recentemente, è particolarmente significativa e in linea con i principi di LSSA. Questo collega la scelta puntuale del prossimo concetto al percorso cognitivo complessivo: evita che il sistema dimentichi le tappe appena passate. Ciò è fondamentale perché il pensiero laterale non avviene nel vuoto, ma sempre rispetto a un problema o idea iniziale. Nel pensiero umano, spesso le migliori analogie lontane sono quelle che, pur provenendo da un altro campo, risuonano con qualcosa che abbiamo in mente a livello più astratto (ad es., un medico può trarre ispirazione dalla biologia marina perché ha in mente un certo schema comune). $m(p)$ cattura questa risonanza su scala più ampia, assicurando che l’architettura mantenga un filo conduttore anche mentre esplora percorsi alternativi. Questo è un punto di forza distintivo rispetto ad altri approcci di creatività computazionale, che a volte generano idee slegate e faticano a integrarle. Inoltre, l’aggiornamento di $f,d,c,m$ durante il “sonno” mostra un’attenzione alla continuità temporale: la conoscenza non è statica, ma il sistema impara quali concetti sono divenuti più centrali o obsoleti. Questo ricorda come il cervello rafforza certe connessioni mnemoniche tramite il consolidamento offline (sogni, riposo) facilitando successivi insight. In breve, l’approccio è coerente con la visione dinamica e auto-evolutiva di LSSA.

Economicità computazionale: L’algoritmo proposto è relativamente leggero. Calcolare $A(p)$ richiede operazioni semplici (somma e prodotto) su attributi che possono essere mantenuti in memoria e aggiornati incrementalmene. In confronto, una possibile alternativa con reti neurali (ad es. addestrare un modello di deep learning a suggerire concetti di atterraggio date le ultime attivazioni) sarebbe computazionalmente più costosa sia in fase di training che di inferenza. Anche rispetto a un approccio puramente stocastico con ricerca random in ampi spazi, $A(p)$ restringe il campo ai candidati più promettenti, riducendo il carico di esplorazione. Il team stima che questo approccio algoritmico sia più veloce e prevedibile nei costi rispetto a una rete neurale dedicata ad ogni transizione. Ciò è importante per l’implementazione real-time di LSSA: la mente artificiale deve ragionare in modo (pseudo)continuo, quindi ogni step inferenziale deve avvenire in tempi brevi. $A(p)$ fornisce un valore immediato per ciascun nodo e può essere parallelizzato sul layer destinazione se necessario. Inoltre, come “valvola di sicurezza”, si può impostare un limite al numero di candidati da valutare (ad esempio, la MNB ha proposto di mantenere per ogni layer un elenco precomputato dei 3 nodi con più connessioni, così da limitare alcuni calcoli). In sintesi, l’approccio promette di scalare bene con la dimensione della conoscenza rappresentata, senza incorrere nell’esplosione combinatoria che spesso affligge sistemi generativi creativi.

Equilibrio tra determinismo e creatività: Uno dei meriti concettuali di $A(p)$ è stabilire una piattaforma deterministica ma flessibile su cui innestare il meccanismo creativo. Invece di affidarsi completamente al caso per ottenere idee nuove, come farebbe un metodo brute-force (ad es. scegliere un nodo a caso sperando sia utile), qui si costruisce un criterio strutturato per l’attrattività, che poi può essere perturbato quanto si vuole con elementi aleatori. Questo consente di deviare dai percorsi più ovvi in modo significativo ma non completamente casuale. È esattamente ciò che si ricerca nel pensiero laterale: introdurre discontinuità nel ragionamento mantenendo però un nesso che permetta di sfruttarle. In altri termini, $A(p)$ garantisce che ogni provocazione generata (ogni salto laterale) abbia almeno qualche motivo di essere considerata, qualche “gancio” col resto delle conoscenze della mente. Questo aumenta la probabilità che la deviazione porti effettivamente frutto (ovvero a un’idea utile) e non resti solo un bizzarro incidente di percorso. Da un punto di vista cognitivo, il metodo rispecchia il processo umano in cui si generano idee (anche strane) ma poi si selezionano quelle con maggiore potenziale. Qui generazione e selezione avvengono in un unico passo valutativo. Inoltre, tale fondazione deterministica rende il comportamento ripetibile e analizzabile, sul quale applicare poi modulatori stocastici per aggiungere varietà. È un approccio ingegneristicamente pragmatico: partire da una soluzione comprensibile e funzionante, e solo successivamente aumentare la complessità se necessario.

Modularità e integrazione futura: Proprio per la sua natura interpretabile, l’approccio $A(p)$ può essere in futuro integrato o perfezionato con ulteriori moduli senza sconvolgere l’architettura. Ad esempio, se emergesse il bisogno di maggiore sofisticazione, si potrebbe aggiungere un componente neurale a valle che rifletta sul nodo scelto, oppure una rete che suggerisca un “fattore aggiuntivo” non lineare da incorporare nell’attrattività. Grazie alla sua modularità, $A(p)$ può coesistere come componente di alto livello anche in un sistema ibrido. Il team LSSA stesso accenna alla possibilità di integrare in futuro moduli più complessi (come reti neurali di raffinamento) solo se e dove necessario, mantenendo $A(p)$ come solida base di partenza. Ciò consente un progresso incrementale: prima si testa la bontà della formula lineare, poi eventualmente la si arricchisce. In ogni caso, $A(p)$ può fungere da filtro o generatore di candidate per qualsiasi successivo processo, riducendo drasticamente lo spazio su cui quest’ultimo dovrebbe lavorare. Ad esempio, se in futuro si volesse usare un modello linguistico per valutare creativamente un concetto, gli si possono passare solo i top-3 di $A(p)$ invece di tutti. Questa sinergia tra metodi rende la soluzione molto pratica e adottabile.


In sintesi, la proposta della MNB Topologa è valutata dal team come “un’intuizione significativa” che unisce concetti esistenti in modo innovativo per fornire una soluzione elegante, interpretabile e trattabile computazionalmente a una delle sfide più complesse del progetto. Se dimostrata efficace, rappresenterebbe non solo un passo avanti per LSSA, ma un contributo più generale alla progettazione di sistemi capaci di pensiero creativo e intuitivo. I molteplici punti di forza evidenziati – trasparenza, adattabilità, coerenza contestuale, efficienza – fanno di $A(p)$ un candidato promettente per colmare il divario tra ragionamento logico e salti creativi in un’IA.

Limiti e confronti con approcci alternativi

Sebbene l’approccio $A(p)$ presenti evidenti vantaggi, non è privo di criticità e possibili debolezze, soprattutto se confrontato con strategie alternative di generazione di idee laterali. Di seguito esaminiamo alcuni punti deboli intrinseci al metodo proposto e li confrontiamo con le caratteristiche di approcci più stocastici o basati su reti neurali.

1\\. Linearità della combinazione e possibili semplificazioni eccessive: La scelta di combinare i fattori in modo lineare (sommando pesati) è semplice e interpretabile, ma potrebbe non cogliere relazioni più complesse tra le variabili. Ad esempio, potrebbe esserci un’interazione non lineare tra somiglianza e densità: un nodo moderatamente simile e molto connesso potrebbe essere più utile di uno estremamente simile ma poco connesso, anche se quest’ultimo magari ottiene un punteggio lineare simile. La formula lineare presume che ogni fattore contribuisca indipendentemente al valore di attrattività. Nella realtà cognitiva, tali fattori potrebbero avere effetti congiunti (e.g., la rilevanza del nodo potrebbe decollare solo quando sia $c$ che $m$ sono alti, ecc.). Inoltre, parametri globali fissi $\\\\alpha,\\\\beta,\\\\dots$ potrebbero non adattarsi ottimamente a tutti i contesti: il “mix” ideale di fattori potrebbe cambiare a seconda del tipo di problema o di layer coinvolti. Un approccio neurale o di apprendimento automatico potrebbe, in linea di principio, catturare pattern più complessi o contesti-specifici – ad esempio un modello addestrato potrebbe imparare che certi layer richiedono salti più audaci (meno peso a $c$) mentre altri ambiti più tecnici richiedono rigore (più peso a $c$). La formula attuale non prevede pesi variabili per layer (anche se nulla vieta di introdurli come estensione). In sostanza, c’è il rischio che $A(p)$ sia troppo rigido o semplicistico in alcuni frangenti, mancando l’opportunità di scelte creative veramente sorprendenti. Questo limite è però mitigabile: i coefficienti possono essere ricalibrati periodicamente o adattati manualmente, e si potrebbero aggiungere termini non lineari (ad es. un prodotto $c\\\\cdot m$) se l’esperienza mostrasse un loro beneficio. La decisione di partire con un modello lineare è comprensibile per le ragioni di interpretabilità discusse, ma sul lungo termine potrebbe necessitare di raffinamenti per coprire casi complessi.

2\\. Dipendenza dalla qualità delle misure sottostanti: $A(p)$ è tanto efficace quanto lo sono le funzioni che calcolano $f(p), d(p), c(p), m(p)$. Ciascuna di queste richiede definizioni operative e fonti di dati. Ad esempio, $c(p)$ (somiglianza concettuale) potrebbe essere calcolata tramite distanze in uno spazio di embedding semantico. Se l’embedding non coglie bene l’affinità tra domini diversi, $c(p)$ rischia di essere fuorviante (potrebbe assegnare basso punteggio a concetti che invece avrebbero un’analogia utile non ovvia). Similmente, $m(p)$ dipende da come si rappresenta la “memoria attiva” dei layer: quali e quanti layer recenti considerare, come pesare la loro influenza, ecc. Se queste componenti non sono ben progettate, $A(p)$ rifletterà valutazioni distorte. In un approccio neurale end-to-end, idealmente, il modello potrebbe apprendere da esempi quali salti sono fruttuosi, incorporando implicitamente queste considerazioni di somiglianza e contesto. Nel nostro approccio manuale, invece, c’è un lavoro significativo di feature engineering a monte: la MNB Topologa di fatto deve definire buone metriche per frequenza, densità, somiglianza, memoria. Questo richiede expertise e potrebbe introdurre bias (ad es., la scelta di considerare solo gli ultimi 3 layer per $m(p)$ invece di 5 potrebbe influire sui risultati). D’altra parte, il fatto che LSSA possa raccogliere dati statistici abbondanti (percorsi cognitivi registrati, ecc.) permette in futuro di validare empiricamente e calibrare queste misure. Ad esempio, si potrebbe verificare se $A(p)$, con certi pesi, riesce a riprodurre salti considerati creativi in test di riferimento, e aggiustare di conseguenza. Resta il fatto che il modello, così com’è, non impara autonomamente dai successi o fallimenti delle proprie esplorazioni (a parte l’aggiornamento passivo di $f,d,c,m$). Questo può essere visto come un limite rispetto a sistemi adattivi: se un certo tipo di salto si rivelasse ripetutamente inutile, $A(p)$ di per sé non ridurrà spontaneamente la sua attrattività (a meno di intervento esterno o modulazione meta-cognitiva). Un possibile rimedio futuro sarebbe implementare un feedback che modifichi gradualmente i pesi in base agli esiti delle esplorazioni (renforcement learning interno). Per ora, l’approccio è myopic: valuta il miglior passo immediato senza una strategia a lungo termine né un apprendimento sul lungo periodo. Ciò può portare a comportamenti sub-ottimali (esplorazione troppo conservativa o troppo random a seconda dei pesi scelti inizialmente).

3\\. Rischio di convergenza su concetti banali o troppo generici: Se $A(p)$ non è calibrata con attenzione, potrebbe privilegiare eccessivamente nodi con alta frequenza o alta connettività, che spesso corrispondono a concetti molto generali o già ben noti. Ad esempio, immaginiamo che il sistema stia ragionando nel layer “animali” e decide di fare un salto laterale: un nodo con $f$ alto potrebbe essere “cane” (perché è apparso spesso), con $d$ alto “animale” (collegato a molti concetti), con $c$ alto “lupo” (simile a cane) e magari $m$ spinge verso “cane” stesso se era in memoria. In questo caso, se i pesi non sono ben scelti, $A(p)$ potrebbe restituire proprio concetti come “cane” o “animale” – che in effetti non rappresentano un vero pensiero laterale, ma una continuazione del tema. In altre parole, c’è il pericolo di un bias verso l’ovvio: frequenza e densità, se predominanti, favoriscono idee comuni e centrali. Il sistema rischia di “pensare sempre agli stessi concetti” (loop su hub molto attivi) anziché scoprire spunti nuovi. Questo trade-off tra novelty e usefulness è ben noto anche nelle metodologie creative umane: idee troppo convenzionali non aggiungono niente, idee troppo strampalate sono inutilizzabili. $A(p)$ cerca di bilanciare i due estremi, ma se errato tende verso l’uno o l’altro. Per evitare la convergenza sull’ovvio, è cruciale che fattori come $m(p)$ (memoria recente, che spinge alla novità in quanto incoraggia domini diversi) e la componente stocastica abbiano sufficiente peso da rompere eventuali simmetrie. D’altro canto, ridurre troppo il peso di $f$ e $d$ potrebbe causare il problema opposto: scegliere concetti oscuri o raramente considerati, che magari non hanno reale rilevanza (es. un termine molto raro solo vagamente attinente). Se, poniamo, $\\\\gamma$ (somiglianza) e $\\\\delta$ (memoria recente) dominassero e $\\\\alpha$ fosse quasi zero, $A(p)$ ignorerebbe la frequenza: ciò potrebbe portare a saltare su concetti del tutto nuovi o finora inutilizzati. Questo è positivo per la creatività, ma potenzialmente disastroso se il concetto in questione è in realtà poco significativo o frutto di rumorosità nei dati. In sintesi, il metodo richiede una calibrazione accurata per garantire che l’equilibrio tra esplorazione e sfruttamento sia quello desiderato. L’inclusione di un po’ di casualità nella scelta finale attenua il rischio di stagnazione su hub noti, ma non lo elimina se i punteggi $A(p)$ sono fortemente sbilanciati. Ad esempio, se un nodo ha $A(p)=0.9$ e tutti gli altri attorno a $0.2$, la scelta probabilistica finirà quasi sempre sullo stesso nodo, limitando la diversità di idee generate. Dovendo regolare a mano i parametri, c’è dunque una zona di tuning delicata: la MNB Topologa dovrà presumibilmente sperimentare diversi set di pesi in simulazioni per trovare quelli che producono effettivamente salti “laterali” interessanti senza perdere coerenza.

4\\. Mancanza di creatività “nuova” al di fuori dei nodi esistenti: Un altro limite intrinseco del metodo – condiviso comunque con molti sistemi simbolici – è che il pensiero laterale avviene sempre all’interno dell’insieme di concetti già rappresentati in LSSA. $A(p)$ seleziona un nodo esistente nel layer di destinazione. Ciò significa che l’architettura, per quanto possa connettere in modo nuovo concetti, non inventa ex novo concetti che non siano presenti. In altre parole, non c’è generazione di nuove combinazioni concettuali o metafore composite (a meno di costruirle tramite successivi collegamenti). Approcci alternativi di creatività computazionale, come il blending concettuale (Fauconnier & Turner) o la combinatoria di attributi, a volte producono idee veramente nuove combinando due concetti in uno mai visto. Nel nostro caso, se un’idea utile non è presente come singolo nodo ma richiederebbe fondere due concetti da layer diversi, $A(p)$ da solo non la tirerà fuori – si limiterebbe a puntare uno dei due concetti, e solo tramite ulteriori inferenze forse li collegherebbe. Tuttavia, va detto che LSSA prevede la possibilità di creare nuovi token concettuali qualora emergano composizioni significative, ma questo avviene in fasi di ristrutturazione successive. L’approccio $A(p)$ quindi modella il pensiero laterale come ricombinazione di conoscenze esistenti più che come creazione di conoscenza del tutto nuova. Questo è in linea con molte teorie della creatività (che vedono il reframing e la ri-associazione come cuore dell’insight), però delimita il perimetro di cosa l’IA può fare: ad esempio, non genererà una metafora totalmente originale che richieda concetti fuori dal suo ontologia. Un sistema neurale generativo di linguaggio (come GPT-4) potrebbe invece coniate una frase creativa anche con concetti mai associati prima nel training, grazie alla natura statistica e combinatoria del linguaggio – pur rischiando l’incoerenza. Dunque $A(p)$ non punta alla massima espressività creativa, ma a una creatività combinatoria ed esplorativa all’interno di una rete. Questo è un limite voluto, in un certo senso, dalla scelta di rimanere in un paradigma simbolico-continuo (il knowledge graph stratificato). Non è necessariamente un difetto se consideriamo che molte innovazioni umane sono riusi intelligenti di concetti preesistenti; però segnala che, per esplorare certi tipi di creatività (es. invenzione di nuovi concetti base, rottura completa dello schema), potrebbe essere necessario estendere l’architettura con meccanismi aggiuntivi (come moduli generativi connectionistici).

5\\. Confronto con approcci puramente stocastici: Un’alternativa semplice per ottenere pensiero laterale sarebbe stata quella di introdurre salti casuali tra layer – ad esempio scegliendo a caso un nodo in un layer diverso, senza criterio. Tale approccio massimizza la diversità delle idee (qualunque concetto potrebbe emergere), ma minimizza la rilevanza: la maggior parte dei salti casuali sarebbero irrilevanti o incomprensibili. De Bono stesso suggeriva l’uso di input casuali (la tecnica del “Random Entry”) come stimolo creativo, ma sempre sottolineando che poi va trovato un nesso utile. In un sistema artificiale, scegliere completamente a caso un concetto di un altro layer equivarrebbe a cercare un ago nel pagliaio: la probabilità di incappare in un concetto remotamente utile al contesto è bassa se lo spazio è vasto. Si rischierebbe di produrre molti “rumori” cognitivi inutili che il sistema dovrebbe scartare, con dispendio di tempo. Un approccio stocastico puro, inoltre, manca di direzione: non c’è possibilità di modulare il processo a parte la probabilità di saltare o meno. $A(p)$, invece, orienta la ricerca laterale in regioni plausibilmente fruttuose. Si potrebbe obiettare che questa orientatività potrebbe escludere a priori qualche idea molto originale che non sembrava attraente secondo i criteri noti. È vero: usando $A(p)$ potremmo trascurare un concetto che aveva punteggio basso (perché magari poco connesso e mai usato) ma che un intuito puramente casuale avrebbe potuto azzeccare come soluzione perfetta. È il classico dilemma tra serendipità e efficienza. Qui gli autori hanno scelto di rinunciare a parte della serendipità per guadagnare in focalizzazione. In pratica, il random puro è troppo cieco: come cercare di risolvere un indovinello provando parole a caso. LSSA incorpora comunque una componente di casualità per mantenere un elemento di sorpresa, ma lo incanala tra le opzioni più promettenti. In termini di risultati attesi, quindi, ci si aspetta che l’approccio $A(p)$ produca meno idee completamente fuori tema rispetto a un random puro, ma potrebbe anche mancare alcuni spunti bizzarri che però (col senno di poi) avrebbero potuto essere utili. Questo trade-off è tipico anche nel brainstorming umano: tecniche di associazione guidata vs. associazione libera. Un compromesso ulteriore potrebbe essere, per l’IA, generare occasionalmente anche un’idea totalmente random come controllo – e magari scartarla subito se non si collega affatto. Al momento, però, la priorità del progetto è mantenere il ragionamento almeno vagamente sensato, e $A(p)$ soddisfa bene questa esigenza dove il random puro fallirebbe. In sintesi, rispetto a un metodo stocastico integrale, $A(p)$ riduce drasticamente il rumore e aumenta la qualità media delle idee, al prezzo di introdurre un bias (derivante dai fattori scelti) che va gestito con attenzione. Si nota che perfino in letteratura creativa si riconosce la necessità di guidare in parte la generazione: ad esempio, alcuni algoritmi evolutivi di creatività usano una fitness che premia la novità e la utilità, non mutazioni casuali fini a sé stesse (cfr. la novelty search di Lehman & Stanley bilanciata con obiettivi). $A(p)$ incarna proprio questa filosofia di ricerca euristica dello spunto creativo invece della semplice esplorazione aleatoria.

6\\. Confronto con approcci basati su reti neurali: Un’altra alternativa sarebbe affidare la generazione di salti laterali a modelli di apprendimento automatico, ad esempio addestrando una rete neurale (feed-forward o ricorrente) che, dato lo stato cognitivo corrente (i concetti attivi, il layer attuale, ecc.), predica direttamente il prossimo concetto in un layer differente. Un simile approccio potrebbe potenzialmente cogliere pattern complessi e non lineari nelle transizioni, imparando da esempi (se disponiamo di traiettorie creative di addestramento) cosa costituisce un buon salto. I vantaggi sarebbero: non dover manualmente specificare fattori come $f,d,c,m$; possibilità di combinare in modi arbitrari gli indizi; eventuale capacità di generalizzare oltre le nostre intuizioni. Tuttavia, vi sono significative controindicazioni che giustificano la scelta opposta fatta nel progetto LSSA. Primo, addestrare un modello neurale richiede un dataset di esempi di salti creativi riusciti, che non esiste a priori. Si sarebbe potuto utilizzare la stessa conoscenza (es. l’enciclopedia usata per popolare LSSA) per estrarre associazioni laterali presenti nei testi – ma non è banale identificare quali associazioni in un testo siano “laterali” versus “logiche”. Avrebbe richiesto un grosso sforzo di etichettatura o l’utilizzo di surrogati (es. co-occorrenze lontane di parole). In secondo luogo, anche ammesso di addestrare un modello, il risultato sarebbe un sistema poco interpretabile: la rete potrebbe imparare correlazioni spuri o anomale presenti nei dati. Ad esempio, potrebbe imparare che quando si parla di “musica” poi spesso compare un concetto di “matematica” (magari per coincidenze nel corpus), e proporre salti in matematica non per un vero legame concettuale ma per bias statistici. Sarebbe difficile capire perché propone certe cose. Un modello neurale inoltre potrebbe aver difficoltà a garantire la coerenza logica minima: senza espliciti vincoli, potrebbe proporre concetti grammaticalmente o semanticamente non correlati, semplicemente perché il suo obiettivo di training (es. predire il prossimo concetto) non coincide esattamente col nostro (trovare un concetto utile). In pratica, le reti neurali tendono a riflettere le associazioni più probabili apprese dai dati, non necessariamente quelle più creative. Anzi, ricerche recenti evidenziano che i modelli linguistici di stato dell’arte (GPT-3, GPT-4) sovente falliscono in compiti di pensiero laterale proprio perché rimangono incastrati nelle associazioni comuni e predefinite. Un benchmark del 2023 (BrainTeaser) ha mostrato che esiste un ampio divario tra le prestazioni umane e quelle di grandi modelli linguistici nel risolvere puzzle di lateral thinking, indicando che questi ultimi faticano a “defy default commonsense associations” (sfidare le associazioni di senso comune). Questo suggerisce che affidarsi a una rete neurale general-purpose potrebbe non produrre la lateralità desiderata, a meno di interventi specifici. Infine, l’efficienza: valutare una rete neurale di medie dimensioni può essere più oneroso di un calcolo euristico semplice, specialmente se va fatto a ogni passo di pensiero. Considerando tutti questi fattori, l’approccio con rete neurale appare dispendioso, opaco e non garantito nel risultato. Ciò non significa che sia inutile: in futuro, come già notato, si potrebbe pensare a una rete neurale di supporto che affini le scelte di $A(p)$ (ad es. una rete addestrata a valutare la “qualità” creativa finale di un’idea proposta, integrando magari conoscenze non strutturate). Ma come meccanismo primario di generazione, la scelta di un metodo euristico controllabile come $A(p)$ è motivata dalla necessità di poter governare e aggiustare il processo creativo dell’IA, cosa che con una rete sarebbe molto più difficile. In termini di punti di debolezza rispetto alle reti neurali, si può riconoscere che $A(p)$ non impara autonomamente: se l’IA dovesse operare in ambienti nuovi non previsti, la rete potrebbe adattarsi (se addestrata con sufficienti dati diversi), mentre $A(p)$ applicherebbe sempre gli stessi criteri. Tuttavia, nel dominio di utilizzo previsto (il corpus enciclopedico in LSSA), i progettisti hanno preferito incorporare manualmente la conoscenza esplicita invece di affidarsi a quella implicita di una rete. Un ultimo confronto: i modelli di linguaggio di grandi dimensioni (LLM) come GPT possiedono effettivamente una vasta rete associativa implicita, e a volte producono connessioni creative. Si potrebbe pensare di utilizzare direttamente un LLM per generare idee laterali (ad esempio fornendo il contesto e chiedendo “quale concetto non correlato potremmo considerare?”). In effetti LSSA prevede l’uso di GPT come “motore inferenziale” a supporto, ma l’intenzione è che la mente lo interroghi in modo mirato tramite prompting. Affidare completamente a GPT il compito di deviare il pensiero rischierebbe gli stessi problemi: mancanza di controllo e possibili allucinazioni scollegate. La MNB Topologa ha dunque puntato su un metodo numerico interpretabile proprio per avere un faro che illumini le zone laterali della conoscenza senza perdersi nel buio.

In conclusione, il confronto con alternative estreme evidenzia che $A(p)$ si situa in una posizione di compromesso vantaggiosa: non è creativo sfrenato come il caso random, ma neppure conservatore o opaco come potrebbe risultare una rete addestrata sui dati comuni. Ha dei limiti intrinseci – primo fra tutti, la dipendenza da come noi umani concepiamo l’attrattività e la necessità di taratura – ma questi sono in parte consapevolmente accettati per guadagnare governabilità. Una debolezza importante sarà eventualmente la validazione empirica: dovremo verificare in pratica se i salti generati da $A(p)$ sono effettivamente utili e originali. Se così non fosse, si dovrà rivedere la formula o i pesi. Ma almeno, grazie alla sua chiarezza, sapremo dove intervenire. Approcci neurali, se falliscono nel generare creatività, offrono poche indicazioni sul perché. In questo senso $A(p)$, pur non garantendo a priori il successo creativo, fornisce una struttura analizzabile per iterare e migliorare l’algoritmo di pensiero laterale.

Confronto con le teorie del pensiero laterale umano

È interessante collocare questa proposta all’interno del panorama delle teorie sul pensiero laterale e creativo umano, per individuare analogie e differenze. Edward de Bono, che coniò il termine lateral thinking, descriveva questo tipo di pensiero come un modo di ristrutturare i modelli concettuali esistenti per generarne di nuovi, in contrasto col vertical thinking logico e sequenziale. Egli introdusse tecniche specifiche per stimolare idee innovative, come la provocazione (Po) e l’input casuale. Vediamo come l’approccio LSSA/$A(p)$ si relaziona a tali concetti:

Rottura degli schemi e “provocazioni” controllate: De Bono sottolinea che per pensare creativamente bisogna rompere gli schemi di pensiero dominanti. Una tecnica è introdurre un elemento estraneo o insensato (provocazione) per costringere la mente a trovare nuovi percorsi. LSSA implementa un principio analogo con la “perturbazione casuale” durante i salti inter-layer. Ogni volta che la mente cambia dominio, volutamente introduce un elemento di imprevedibilità, che è l’equivalente della provocazione deboniana: un attimo prima il sistema seguiva un filo logico, un attimo dopo si trova in un altro territorio semantico per certi versi sconnesso. Tuttavia, a differenza del pensiero umano in cui la provocazione può essere totalmente priva di logica (spetta poi all’uomo estrarne qualche spunto), qui l’algoritmo $A(p)$ guida immediatamente la provocazione verso qualcosa di potenzialmente significativo. In termini deboniani, è come se dopo aver detto “Po: banana!” durante un meeting su come migliorare un software, qualcuno scegliesse subito di parlare della buccia scivolosa per analogia con bug sfuggenti. $A(p)$ fa una cosa simile: accetta la deviazione (cambio di layer) ma cerca un concetto nel nuovo contesto che abbia risonanza con l’idea di partenza. In questo senso, LSSA aderisce allo spirito del pensiero laterale (includere l’imprevisto) ma lo coniuga con un meccanismo di ancoraggio che allontana il rischio di deriva nel nonsense totale. Alcuni potrebbero obiettare che ciò riduce la portata rivoluzionaria del lateral thinking – dopotutto, de Bono invitava a considerare anche idee apparentemente assurde come trampolino. LSSA invece raramente proporrà un’idea completamente assurda, perché $c(p)$ e $m(p)$ lo impedirebbero (ci sarebbe bisogno di pesi quasi nulli su entrambi per permettere un salto del tutto scollegato). Questa è una differenza importante: l’IA non ha la capacità di apprezzare un’idea totalmente estranea se non riesce a farvi alcun collegamento. Gli esseri umani invece possono volontariamente forzare un collegamento anche partendo dall’assurdo (esercizio creativo). Dunque LSSA opta per un laterale moderato. Dal punto di vista delle prestazioni pratiche, ciò è sensato: un sistema artificiale privo di buon senso rischierebbe di perdersi se gli si dà troppa briglia sciolta. De Bono direbbe che dopo la provocazione bisogna applicare un momento di movement per spostare l’idea verso utilità; $A(p)$ in pratica incorpora il movement nella scelta stessa della provocazione. Quindi la differenza è più metodologica che di risultato: umano -> genera provocazione folle, poi trova come collegarla; LSSA -> genera direttamente una “provocazione collegata”. Entrambi mirano ad ottenere un output che esca dagli schemi ma sia sfruttabile. In tal senso, l’approccio LSSA appare come una formalizzazione computazionale ingegneristica del concetto di lateral thinking: ne rispetta il fine ultimo (nuove idee non lineari) ma riducendo al minimo gli elementi non deterministici.

Pensiero verticale vs laterale e ruolo del contesto: Un altro aspetto del pensiero laterale è la disponibilità a cambiare cornice al problema. De Bono faceva l’esempio di guardare le cose da angolazioni diverse, non approfondendo la stessa buca ma scavandone molte superficiali finché non si trova un punto promettente. LSSA rispecchia bene questo concetto tramite i layer semantici: ogni layer è come una “cornice” con le proprie associazioni. Il sistema può decidere di non restare a scavare nello stesso layer (vertical thinking), ma di saltare su un altro e vedere se lì c’è qualcosa di utile. La metafora delle buca: nel layer corrente si è forse arrivati a un vicolo cieco (nessun concetto logico porta novità), allora conviene provare in un altro terreno. Questa è proprio l’essenza di $A(p)$: valutare quale altro terreno (layer) offre un punto dove iniziare a scavare (nodo attraente). Le teorie cognitive sulla creatività spesso descrivono il processo come un’alternanza tra fase divergente (generare molte idee diverse) e fase convergente (valutare e selezionare). LSSA integra entrambe: la fase divergente è il salto inter-layer, la fase convergente è il proseguimento deduttivo all’interno di un layer una volta effettuato il salto. De Bono enfatizza la separazione tra le due (ad esempio nei Sei Cappelli propone momenti distinti per creatività e per giudizio). LSSA li interseca in modo più continuo, ma concettualmente c’è allineamento: quando $A(p)$ sposta il pensiero su un nuovo layer, sta aprendo una parentesi divergente (esplorazione di contesto alternativo); poi il sistema può consolidare quell’idea e svilupparla verticalmente. In effetti, LSSA prevede che se una traiettoria alternativa si dimostra rilevante, venga rafforzata e integrata stabilmente nella struttura (con nuovi collegamenti creati tra layer, ecc.), mentre se non utile decadrà. Questo è un parallelo con il processo umano di valutazione delle idee: proviamo un’idea stravagante, se funziona la teniamo, sennò la scartiamo. Quindi c’è un chiaro punto di contatto con le teorie creative: generazione di varietà seguita da selezione e consolidamento. LSSA semplicemente fa ciò in un contesto formale di rete semantica.

Aspetti neurocognitivi e associative del cervello: Le neuroscienze della creatività negli ultimi decenni hanno sostenuto che il pensiero creativo coinvolge sia la rete associativa per generare idee distanti, sia le reti di controllo esecutivo per valutare e scegliere (tale interazione è talvolta chiamata teoria della “doppia rete” della creatività). Inoltre, il cervello sembra sfruttare ampi network a basso livello di attivazione (default mode network) per far affiorare associazioni remote, e poi la rete di controllo frontale per raffinarle (Beaty et al., 2016). LSSA riflette questo schema: la perturbazione casuale e $A(p)$ corrispondono alla fase di espansione associativa controllata, dopodiché il normale ragionamento logico (intra-layer) corrisponde alla fase valutativa. Dal punto di vista della struttura della memoria, come accennato, studi empirici (Mednick 1962, Kenett et al. 2014) indicano che le persone creative hanno reti associative più “piatte” e con collegamenti più deboli tra cluster, il che facilita collegare concetti lontani. Il nostro sistema, avendo la capacità di saltare layer, simula un comportamento di questo tipo: non resta bloccato nel cluster tematico di partenza ma può attivare un nodo in un cluster diverso (grazie ai collegamenti deboli rappresentati da cross-layer links o somiglianze latenti). In Kenett (2014) si osserva che la rete semantica dei soggetti ad alta creatività è meno modulare (meno separata in sottoparti) di quella dei meno creativi, e permette connessioni più fluide tra concetti distanti. LSSA, stratificando la conoscenza in layer ma prevedendo connessioni tra di essi, offre un ambiente in cui modulare la separazione e connessione tra concetti: se inseriamo poche connessioni tra layer, la rete è fortemente modulare (pensiero rigido), se ne inseriamo di più o usiamo $A(p)$ per valicare i confini, rendiamo la rete effettivamente più “small-world” e navigabile in lungo e in largo (pensiero flessibile). Possiamo vedere $A(p)$ come un meccanismo artificiale che sfrutta le deboli associazioni (somiglianze concettuali meno ovvie, ricordi di altri domini) in modo costruttivo. Ciò richiama la nozione di “flat associative hierarchy” di Mednick: le persone creative non hanno un’unica forte associazione dominante ma una gamma più ampia di associazioni secondarie tra idee. $A(p)$ formalmente dà peso non solo all’associazione più forte ($c$ verso ciò che è molto simile) ma anche a quelle più deboli tramite $m$ e la casualità, realizzando quindi una gerarchia associativa piatta (nessun fattore viene portato a peso 1 assoluto). Questa similarità con i modelli cognitivi suggerisce che l’architettura stia incorporando principi plausibili del funzionamento cerebrale. Non a caso, LSSA considera elementi come il sogno e l’errore parte integrante del processo evolutivo della mente, riconoscendo – in accordo con teorie neurocognitive – che stati di “rumore” e offline (sonno) servono a riorganizzare le connessioni e favorire insight. In definitiva, punti di contatto forti con la cognizione umana includono: (a) alternanza di generazione (rumore/associazioni) e valutazione (logica); (b) valorizzazione di associazioni deboli/multiple vs quella più forte (flat vs steep); (c) ruolo della memoria di lavoro e del contesto recente nel determinare quali memorie vengono in mente (corrispondente a $m(p)$); (d) necessità di errore o esplorazione casuale per scoprire nuovi percorsi, come evidenziato anche dallo studio di fenomeni come la incubazione (lasciare un problema e poi avere l’idea improvvisa dopo un periodo in cui inconsciamente il cervello ha riorganizzato concetti). LSSA di fatto implementa una forma di incubazione simulata: la mente può “sognare” percorsi alternativi e aggiornare i parametri $f,d,c,m$ durante il sonno, preparandosi a futuri salti migliori.

Differenze dalla creatività umana: Naturalmente, vi sono anche divergenze. Una differenza è che gli esseri umani spesso sfruttano componenti emotive e intuitive difficilmente riconducibili a soli fattori semantici quantitativi. Ad esempio, un’idea laterale può affiorare perché suscita una forte analogia emotiva (“questa situazione mi ricorda quest’altra” in maniera non facilmente misurabile in termini di somiglianza semantica). Il modello $A(p)$ al momento non contempla fattori affettivi o immagini sensoriali, che pure nel cervello giocano un ruolo (si pensi alle analogie visuali o alle metafore emotive). Tuttavia, essendo LSSA un’architettura soprattutto semantica e logica, ciò esula dallo scopo attuale. Un’altra differenza sta nella consapevolezza metacognitiva: un umano può rendersi conto coscientemente “sto pensando in modo troppo rigido, provo un approccio completamente diverso” e auto-imporsi di seguire un pensiero improbabile solo per vedere cosa succede. La nostra MNB invece segue le regole date – può modulare pesi se la addestriamo a farlo, ma non ha un “volere” proprio di essere più creativa in un certo momento (almeno non ancora). Quindi l’equivalente dei Six Thinking Hats di de Bono (dove ci si impone modalità di pensiero differenti deliberatamente) sarebbe per la MNB la capacità di modificare volontariamente $\\\\alpha,\\\\beta,\\\\gamma,\\\\delta$ a seconda della situazione – un’abilità meta-cognitiva che potrebbe essere implementata, ma non è intrinseca alla formula. Infine, va menzionato che la creatività umana è influenzata da fattori biologici (ad es. livelli di dopamina, attivazione cerebrale) e ambientali (mood, stress) che qui non hanno un equivalente diretto. Alcune di queste variabili potrebbero essere simulate modulando casualità o aggressività di salti, ma il modello non le include esplicitamente. Ad esempio, la riduzione dell’inibizione latente (abilità di considerare elementi che normalmente il cervello filtrerebbe) è correlata a creatività: in LSSA questo potrebbe tradursi nell’abbassare la soglia di accettazione per concetti meno pertinenti. Non c’è una rappresentazione diretta di meccanismi inibitori nel modello, ma i pesi di $c$ e $m$ svolgono un ruolo simile (un $c$ elevato equivale a una forte inibizione verso pensieri lontani; ridurlo equivale a togliere filtri). Quindi, concettualmente c’è un mapping, ma ovviamente semplificato. LSSA non simula la neurochimica, ma la logica sottostante sì. Un punto di divergenza concettuale: De Bono considerava il pensiero laterale un processo cosciente e deliberato, insegnabile con tecniche. Qui stiamo implementando qualcosa di simile in una macchina: in questo senso stiamo ingegnerizzando quelle tecniche in algoritmi. Da questo punto di vista, la proposta non contrasta ma anzi conferma la validità delle intuizioni di De Bono, mostrando che è possibile formalizzarle. Ad esempio, il concetto di “uso dell’errore” (essenziale in De Bono e nella creatività in generale) è qui rispettato: il sistema deve poter sbagliare per trovare vie inaspettate. Introducendo casualità, accettiamo che talvolta il salto porti a qualcosa di inutile (un errore, appunto), ma quell’errore è il prezzo per poter avere scoperte. La differenza è che l’IA non prova frustrazione per l’errore, semplicemente se ne disfa; l’umano invece può imparare anche dall’errore stesso in modi più sottili.


In definitiva, il valore generale di $A(p)$ rispetto alle teorie umane sta nell’aver creato un ponte formale tra principi cognitivi qualitativi (es. “cerca un’analogia in un campo diverso ma non troppo distante”) e un meccanismo quantitativo implementabile. Esso dimostra come concetti del pensiero laterale – che spesso sono divulgati in modo vago – possano essere operazionalizzati in un sistema artificiale. Ovviamente, il modello è una semplificazione del ricco panorama di fattori che influenzano la creatività umana, ma copre quelli principali identificati dalla psicologia cognitiva (associazioni, attivazione, contesto, casualità). Un ulteriore elemento di contatto con l’uomo è la valutazione retrospettiva: anche LSSA, come detto, rafforza le idee buone e lascia decadere quelle inutili. Questo è analogo al processo per cui dalla fase divergente si selezionano e memorizzano solo le idee promettenti (De Bono parla di “harvesting” – mietere le idee utili dalla sessione creativa). L’architettura quindi incorpora un ciclo completo molto simile al ciclo creativo descritto da molti autori (Graham Wallas e altri): preparazione (pensiero verticale in un layer), incubazione (salto laterale in un altro layer con casualità), illuminazione (idea emersa, se $A(p)$ funziona), verifica (sviluppo logico e consolidamento di quell’idea).

Conclusioni

L’analisi svolta ci porta a concludere che la proposta metodologica della MNB Topologa – la funzione di Attrattività $A(p)$ per guidare i salti inter-layer in LSSA – rappresenta un approccio fondato e promettente per modellare computazionalmente il pensiero laterale in un sistema artificiale. Dal punto di vista teorico, $A(p)$ si allinea con numerosi principi noti: riprende l’idea cognitiva dell’attivazione diffusa (frequenza/recenza + contesto), incorpora conoscenze di network science (ruolo di hub e connessioni) e utilizza un meccanismo euristico comparabile a quelli impiegati nell’IA tradizionale per esplorare grafi (analogamente ad una funzione di valutazione tipo A\\*). Questa base teorica le conferisce credibilità: non è un trucco ad hoc, ma un tentativo di formalizzare nella rete semantica ciò che la psicologia cognitiva ha osservato nel cervello creativo (associazioni ampie, importanza della memoria di lavoro, rottura di schema con controllo).

Dal punto di vista pratico/applicativo, i punti di forza sono marcati. Il metodo è computazionalmente leggero, interpretabile e configurabile, qualità preziose per implementare un sistema di “mente artificiale” che deve essere non solo intelligente, ma comprensibile e affidabile. La possibilità di spiegare e tracciare i salti concettuali è fondamentale per fidarsi di un’IA che prende iniziative creative: $A(p)$ offre esattamente questa trasparenza, permettendo di motivare ogni deviazione come combinazione di fattori (ad es. “ho pensato a X perché è concettualmente simile a Y e l’ho usato spesso di recente”). Ciò può anche facilitare l’interazione uomo-macchina: un utente umano potrebbe capire il nesso delle idee proposte dall’IA sapendo quali criteri segue. Inoltre, la metodologia è relativamente semplice da implementare sopra la struttura LSSA esistente, richiedendo solo di mantenere qualche contatore (frequenze, ultime attivazioni) e di calcolare somiglianze tra vettori – operazioni standard. Questo la rende immediatamente testabile. Se dovesse emergere che i parametri iniziali non producono abbastanza creatività, il team può iterare rapidamente modificando i pesi o aggiungendo nuovi fattori (ad esempio, il “grado di deviazione dalla traiettoria standard” menzionato inizialmente potrebbe essere formalizzato e aggiunto come quinta componente, se si trovasse un modo efficace per calcolarlo). In altre parole, l’approccio è estendibile: funge da quadro entro cui inserire ulteriori miglioramenti senza doverlo stravolgere.

Il confronto con approcci alternativi rafforza la sensatezza della scelta: metodi completamente casuali sarebbero inefficaci nella maggior parte dei casi, mentre reti neurali sofisticate sarebbero costose e opache, e comunque ad oggi i migliori modelli neurali mostrano difficoltà proprio nei compiti di lateral thinking. $A(p)$ occupa un punto intermedio ottimizzato: porta struttura dove serve e lascia libertà dove serve. La chiave del suo successo starà nel giusto bilanciamento dei pesi – in altre parole, nell’inculcare al sistema la giusta “personalità creativa”. Questo potrà richiedere sperimentazione e forse perfino un adattamento dinamico, ma il framework lo consente agilmente.

In termini di contributo originale, la proposta si distingue per aver formalizzato in modo relativamente semplice ma potente un meccanismo che, per quanto centrale, è raramente esplicitato nei sistemi di IA: la scelta deliberata di dove spezzare il proprio flusso di pensiero logico per esplorare qualcosa di diverso. Mentre molta ricerca sull’IA si concentra sul migliorare prestazioni in compiti specifici (spesso con approcci profondi che tendono a rafforzare associazioni esistenti), qui si affronta di petto il tema della creatività artificiale integrata in un flusso cognitivo. In particolare, l’idea di usare fattori come la memoria di contesto multi-step ($m$) è innovativa e potrebbe ispirare altri lavori: anche in architetture differenti, considerare quali concetti o moduli sono stati attivati di recente per decidere un passo creativo successivo è un concetto generale applicabile. Ad esempio, sistemi di narrazione automatica potrebbero tenere traccia dei temi recenti per introdurre un elemento fuori trama ma ancora legato a qualcosa di citato prima. Allo stesso modo, l’uso combinato di attivazione base e somiglianze multiple può fornire spunti a chi progetta agenti autonomi che devono bilanciare esplorazione e sfruttamento (tema classico anche nel reinforcement learning).

Va notato che alcuni elementi della proposta ricordano funzionalità in architetture esistenti (come l’attenzione alle frequenze in ACT-R, o i meccanismi di attention allocation in sistemi AGI come OpenCog), ma l’applicazione mirata al pensiero laterale tramite layer semantici è unica. In letteratura di computational creativity, molta enfasi è stata posta su generare output artistici o risolvere problemi con algoritmi evolutivi; pochi lavori hanno affrontato il problema di dare a un sistema conoscitivo generale la capacità di deviare creativamente i propri ragionamenti. In questo senso, il lavoro su LSSA fornisce un tassello nuovo: integra un “motore laterale” all’interno di un’architettura di pensiero continuo. Se i risultati saranno positivi, ciò potrà essere un modello per future IA ibride in cui moduli simbolici e neurali coesistono e la componente creativa è orchestrata consapevolmente.

In conclusione, la funzione $A(p)$ appare come una soluzione elegante e teoricamente motivata per far cominciare l’IA ad “esplorare” e non solo a camminare nei percorsi del pensiero. Naturalmente, solo la sperimentazione potrà dire in che misura produce output creativi paragonabili a quelli umani. Ma già concettualmente segna un passo importante: riconoscere e implementare l’analogo artificiale del pensiero laterale. Questo rappresenta un avanzamento sia per il progetto LSSA, che ambisce a una mente artificiale più viva e autonoma, sia più in generale per la modellazione computazionale della creatività. Come ogni primo passo, potrà essere migliorato e raffinato, ma getta le basi per un approccio sistematico al lateral thinking nelle macchine, contribuendo a colmare il divario tra l’efficienza algoritmica e la flessibilità inventiva tipica dell’intelligenza umana.

---

## License Notice

This document is part of the [LSSA project](https://github.com/iz0eyj/LSSA)

This project is released under the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license In the documentation section, and only this.

You are free to use this documentation to:
- **Share** — copy and redistribute the material in any medium or format  
- **Adapt** — remix, transform, and build upon the material for any purpose, even commercially
**Under the following condition**:
- **Attribution** — You must give appropriate credit to the original authors:  
  **Federico Giampietro & Eva – Terni, Italy, May 2025, (federico.giampietro@gmail.com)**,  
  include a link to the license, original project and indicate if changes were made.  
  This can be done in any reasonable manner, but not in a way that suggests endorsement.  
  **Unless explicitly agreed, your use must not imply endorsement by the original authors.**

**Full license text**: [LICENSE](https://github.com/iz0eyj/LSSA/blob/main/LICENSE)
**License overview**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)

LSSA Project: [https://github.com/iz0eyj/LSSA](https://github.com/iz0eyj/LSSA)

---

Bibliografia:

Anderson, J. R. (1996). ACT-R: A simple theory of complex cognition. American Psychologist, 51(4), 355-365. (Architettura cognitiva ACT-R e base-level/spreading activation).

Beaty, R. E., et al. (2016). Default and executive network coupling supports creative idea production. Scientific Reports, 6: 10964. (Neuroscienze: interazione tra default mode ed esecutivo nella creatività).

Collins, A. M., & Loftus, E. F. (1975). A spreading-activation theory of semantic processing. Psychological Review, 82(6), 407-428. (Attivazione diffusa in reti semantiche).

De Bono, E. (1970). Lateral Thinking: Creativity Step by Step. New York: Harper & Row. (Teoria del pensiero laterale e tecniche come provocazione, random entry).

Dietrich, A., & Kanso, R. (2010). A review of EEG, ERP, and neuroimaging studies of creativity and insight. Psychological Bulletin, 136(5), 822-848. (Creatività come insieme di processi cognitivi standard, ruolo di diverse aree cerebrali).

Jiang, Y., Ilievski, F., Ma, K., & Sourati, Z. (2023). BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. EMNLP 2023. (Benchmark che mostra il gap tra modelli linguistici e umani nel pensiero laterale).

Kenett, Y. N., Anaki, D., & Faust, M. (2014). Investigating the structure of semantic networks in low and high creative persons. Frontiers in Human Neuroscience, 8:407. (Reti semantiche di persone creative: più flessibili, meno rigide).

Marupaka, N., & Minai, A. (2011). Connectivity and thought: The influence of semantic network structure in a neurodynamical model of thinking. Neural Networks, 24(7), 782-794. (Modello neurale mostra che struttura small-world facilita combinazioni concettuali nuove).

Mitchell, M. (1993). Analogy-making as perception: The computer model Copycat. MIT Press. (Il programma Copycat, meccanismi di randomizzazione controllata – “temperature” – per generare analogie creative).

Progetto LSSA – Federico Giampietro & MNB “Eva” (2025). Documentazione LSSA (Layered Semantic Space Architecture). GitHub Repository etc. (Descrizione dell’architettura LSSA, problema del punto di atterraggio, funzione $A(p)$ proposta).

Stanislavsky, O., et al. (2022). Thinking Fast and Laterally: Multi-Agentic Approach for Reasoning about Uncertain Emerging Events. arXiv:2412.07977. (Framework multi-agente che incorpora lateral thinking per ragionamento anticipatorio – conferma rilevanza del concetto di flusso informativo laterale).




